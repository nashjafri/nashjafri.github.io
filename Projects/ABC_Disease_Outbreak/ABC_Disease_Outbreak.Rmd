---
title: "Approximate Bayesian Computation for Disease Outbreak"
author:
- Nasheed Jafri
- Abigail Watkins
output:
  html_document: default
  pdf_document: 
    latex_engine: xelatex
fontsize: 11pt
---

**GitHub repository:** [StatS610FinalProjectABCDisease](https://github.com/aswatkin/StatS610FinalProjectABCDisease)

## Introduction

In this project, we apply ABC to fit parameters in a model for influenza A and B strains, based on data from past outbreaks in Michigan and Seattle. The process involves drawing parameters from prior uniform distribution, simulating data based on those parameters, computing a similarity measure between simulated and observed data, and iteratively refining the parameter estimates. 

We focus on the 4-parameter model with parameters $(q_{c1}, q_{h1}, q_{c2}, q_{h2})$, which represents the hypothesis that each outbreak has its own infection avoidance rates. Later, we compare this model to a 2-parameter model with parameters $(q_c, q_h)$ for the two outbreaks of the same strain of virus (influenza A in Tecumseh, Michigan [3]). The 2-parameter model hypothesizes that outbreaks of the same virus in a given region have same infection avoidance rates. Finally, we also compare the 4-parameter model to a 3-parameter model with parameters $(q_{c1}, q_{c2}, q_h)$ for the outbreaks of different strains of virus (influenza A and influenza B in Seattle, Washington [4]). The 3-parameter model assumes that outbreaks of different viruses in a given region have same household infection avoidance rates, but different community infection avoidance rates. 

**Objective**: The objective of this project is to replicate the model selection results presented in Section 3.3 of Tony and Stumpf's paper, "[Simulation-based model selection for dynamical systems in systems and population biology](https://watermark.silverchair.com/bioinformatics_26_1_104.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA3swggN3BgkqhkiG9w0BBwagggNoMIIDZAIBADCCA10GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMjV15kHzFknWstbVZAgEQgIIDLvGZcFAgMxJ2FtxTGrsPvkAoO-imsFvwyY2RJRbPWpz_WYOR3ZYIIXnJpmCp_pOOlhB9fGwvPCCBNkFN7jjQvo-jtSs3vYGT9U9ABO1ngcGxGq0M_-xfz5QDNcfLJpdHphXjvXPmNQKw-FbmW7Z-lM4VADhWgRMXeAa69IcQWbf3O3M4YVlAfOhNibTRLt8QLpayutZlbZAwX6aC2a13wmjnKF6Vx3WJWazbewssqJov9CmNXprFKUqnhcq1QLZ4oaGSKYaxVFpmwB2ZylzUBbliQ3fYN6VRAfleLXrmyvOymid2GtXNnhrslyx6SN2OSgbXU0YIgfSgCk5OaCETsFY7VMGzLCuUTB776n6hDJKcZ-Hb7RelIJxLeOZteaxRVOiu-a9pG5NbQQuueQtS0C-kqHlVksEwUAucqzS9UXX3ucvmsIgYK-jQQ8jmtqPTjVkdFGhR1J3LzOw7VJCJQy4b_a_WZLDNS7bskxvvZgU7DOZAVHxYu1aPUHh3UaeJ-5oMwJ-sqFWg_6ZruUPk4L9f1KB1siRgSmxw-Eo4JHKXjSEsIXAylD3m_trgxEIxkeqgXFJ867U-qJxeG39ToS9BptAG_IGK-HfMD0ovPK9mKHXvrp32fRO5S0oiqaCMa8kV4DGwbZjaMArJDV9Ps3WNw_EE2E8m7J4UjiqLNQkihUtUM6d4xmJ-S4zo-qPJkr0ajWkDhQwkeJ1wsaYGXItivcoAB4lzyQmG3Zs5kQQIIa2m4hveEf2mDlglHMoPHTAN5hGG-9_LegexhFcKAZTguF4nNpozqAVsIQaj8DeAaHWY8AvjP5HjDTgYHs4ni3w7EjULGDSroFhBndTpCAMNjtY9yIqoh248Bf7ayWtBCXUx1yJyIamAPGeHej3nPnf80TACr2Of6fJicQ-hFcdVGzQj8qiq8b9GuOFFJ43SnZudftdAwwlA0mQb30ZhvgJvsYngGY752-NegVEd2F_r6N2Jkw-G-fzEnoObf6OXzGaYlNSq4_s4vGnQ38HGe_HPk7fdDIESUA35j1SbK5K83274IWvReND0Byubb2MmgZ4fJ90sCCKnjLeu9ho)" and demonstrate how ABC can be used to estimate model parameters for complex epidemic models. In particular, we recreate Figures 3(a) and 3(c) using a 4 parameter model on two sets of observed data - Table 2 (Addy et al., 1991) and Table 3 (Longini and Koopman, 1982) from the [Supplementary Data](https://academic.oup.com/bioinformatics/article/26/1/104/182571). We also recreate Figures 1(a) and 1(b) from the Supplementary Data.

## The 4-parameter model

Let $q_c$ denote the probability that a susceptible individual does not get infected from the community and $q_h$ the probability that a susceptible individual escapes infection within their household. The probability $w_{js}$, that $j$ out of the $s$ susceptible individuals in a household become infected, is then given by

$$w_{js} = \binom{s}{j}w_{jj}(q_cq_h^j)^{s-j}\, \, , s = 1,2,\dots \, \, , j = 0, 1, \dots , s$$
where $w_{0s} = q_c^s$ for $s = 0,1,2,\dots$ and $w_{ss} = 1 - \sum_{j = 0}^{s-1}w_{js}$.

We aim to infer the pair of parameters, $q_h$ and $q_c$, from the model using data from Supplementary Tables 2 and 3. 

### Prior distribution

Prior distributions of all parameters are chosen to be uniform over the range [0,1].

**Prior distribution:** $q_{c1}, q_{h1}, q_{c2}, q_{h2} \sim \text{Uniform}[0,1]$

In the following code, we pick 4 independent parameters from the distribution Uniform[0,1]. This returns a named list of length 4, which we use to later extract parameters $q_{c1}, q_{h1}, q_{c2}, q_{h2}$. 

```{r}
prior_distribution <- function() {
  parameters <- runif(n = 4, min = 0, max = 1)
  names(parameters) <- c("q_c1", "q_h1", "q_c2", "q_h2")
  as.list(parameters)
}
```

### Probability distribution $w_{js}$ 

We define a function that takes parameters $q_c$, $q_h$ and (maximum) household size, and returns the probability distribution matrix `w_js_matrix` with entries $w_{js}$ for $j = 0,1,\dots, s$, $s = 1 ,2, \dots,$household_size. 

```{r}
# Function to compute w_js matrix for a given outbreak
W_js_matrix <- function(q_c, q_h, household_size) {
  
  # Throw error if q_c or q_h is not a probability
  if ((q_c < 0) || (q_h < 0) || (q_c > 1) || (q_h > 1)) {
    stop(paste("Invalid inputs:", "q_c =", q_c, "b =", q_h, "; Probabilities must be between 0 and 1."))
  }
  
  # Columns (susceptible individuals) indexed by s = 1,..., household_size
  nCol <- household_size      
  # Rows (infected individuals) indexed by j = 0, 1,..., s
  nRow <- household_size + 1  
  
  # Initialize a (zero) matrix to store the probabilities w_js
  w_js_matrix <- matrix(0, nrow = nRow , ncol = nCol)
  
  for (s in 1:nCol) {  # Iterate over number of susceptible individuals in a household
    for (j in 0:s) {   # Iterate over number of infected individuals
                       # j corresponds to row (j + 1) of the w_js matrix
      if (j == 0) {    
          w_js_matrix[j + 1,s] <- q_c^s   
      } else if (j < s) {                 
          w_js_matrix[j + 1, s] <- choose(s, j) * w_js_matrix[j + 1, j] * (q_c * q_h^j)^(s - j)
      } else {                           
          w_js_matrix[j + 1, s] <- 1 - sum(w_js_matrix[1:j,s])
      }
    }
  }
  
  rownames(w_js_matrix) <- paste("j =", 0:household_size)
  colnames(w_js_matrix) <- paste("s =", 1:household_size)
  
  return(w_js_matrix)
}
```

### Simulating data from the distribution $w_{js}$

In this section, we simulate data based on the probability distribution $w_{js}$. The function below takes as inputs the parameters $q_c$, $q_h$, (maximum) household size, and the number of households (for each household size, as a vector), and returns a matrix of simulated data. The simulation is carried out by sampling the number of infected individuals for each household size and store their counts in a matrix.

```{r}
simulate_household_data <- function(q_c, q_h, household_size, n_households) {
  
  # Throw error if user doesn't provide number of households for each household size
  if (length(n_households) != household_size) {
    stop("Invalid inputs: length(n_households) does not match (maximum) household_size.")
  }
  
  w_js_matrix <- W_js_matrix(q_c, q_h, household_size)
  
  # Columns (susceptible individuals) indexed by s = 1,..., household_size
  nCol <- household_size   
  # Rows (infected individuals) indexed by j = 0, 1,..., s
  nRow <- household_size + 1 
  
  # Initialize a (zero) matrix to store the simulated data
  simulated_data <- matrix(0, nrow = nRow, ncol = nCol)
  
  for (s in 1:nCol) {                           
                                          
    probabilities <- w_js_matrix[1:(s + 1), s]  # (j+1) corresponds to row j of w_js_matrix
    
    # Simulate household infections using random sampling
    samples <- sample(0:s, size = n_households[s], replace = TRUE, prob = probabilities)
    
    # Count the occurrences of each infection level
    counts <- table(factor(samples, levels = 0:s))
    
    # Store the simulated counts in the matrix
    simulated_data[1:(s + 1), s] <- as.numeric(counts)
  }
  
  rownames(simulated_data) <- paste("(Infected) j =", 0:household_size)
  colnames(simulated_data) <- paste("s =", 1:household_size)
  
  return(simulated_data)
}
```

We use the `sample` function to generate the samples, but the `rmultinom` function can also be used for the same purpose. 

**Example:** The following is an example of one simulated dataset, given parameters $q_c = 0.8$, $q_h = 0.6$, maximum household size of 5 and number of households same as in Table 2:

```{r}
q_c <- 0.8                                # Probability of avoiding community infection
q_h <- 0.6                                # Probability of escaping household infection
household_size <- 5                       # Maximum number of susceptible individuals in a household
n_households <- c(79, 105, 48, 44, 11)    # Number of households (same as observed data in Table 2)

simulated_data <- simulate_household_data(q_c, q_h, household_size, n_households)
print(simulated_data)
```

### Similarity measure between the simulated and observed data

To apply ABC, we use the following distance function for observed and simulated datasets.

$$d(D_{obs}, D^*) = \cfrac{1}{2}\left(\|D_1−D^∗(q_{c1},q_{h1}) \|_F + \| D_2−D^∗(q_{c2},q_{h2}) \|_F \right) \, ,$$
where 

- $D_{obs} = D_1 \cup D_2$ with $D_1$ and $D_2$ the datasets from different outbreaks

- $D^*$ is the simulated data

- $\| \, . \|_F$ denotes the Frobenius norm of a matrix defined by $\|A\|_F = \sqrt{\sum_{i,j}|a_{ij}|^2}$.

We define the said distance function that takes two pairs of observed and simulated data and returns the average Frobenius distance between them. 

```{r}
frobenius_norm <- function(A) return(sqrt(sum(A^2)))

avg_frob_distance <- function(given_data1, given_data2, simulated_data1, simulated_data2){
  
  # Throw error if given_data1 and simulated_data1 aren't the same size
  if(ncol(given_data1)!=ncol(simulated_data1) || nrow(given_data1) != nrow(simulated_data1)){
    stop("Invalid inputs: given_data1 and simulated_data1 have different dimensions")
  }
  
  # Throw error if given_data2 and simulated_data2 aren't the same size
  if(ncol(given_data2)!=ncol(simulated_data2) || nrow(given_data2) != nrow(simulated_data2)){
    stop("Invalid inputs: given_data1 and simulated_data1 have different dimensions")
  }
  
  distance1 = frobenius_norm(given_data1-simulated_data1)
  distance2 = frobenius_norm(given_data2-simulated_data2)
  total_distance = (distance1+distance2)/2
  return(total_distance)
}
```

### Generating ABC samples

The ABC algorithm for this project is given below.

**Inputs:**

* 4-parameter vector: $q = (q_{c1}, q_{h1}, q_{c2}, q_{h2})$

* Target posterior: $P(q|D_{obs}) \propto P(D_{obs} | q)P(q)$

* A way of simulating from $P(D_{obs}|q) \sim w_{js}$

* Prior on the parameters: $P(q) \sim \text{Uniform}[0,1]$

* Similarity measure: $d(D_{obs}, D^*) = \frac{1}{2} \left( \| D_1 - D^*(q_{c1}, q_{h1}) \|_F + \| D_2 - D^*(q_{c2}, q_{h2}) \|_F \right)$

* Tolerance $\epsilon$

**Sampling:** for $i = 1, 2, \dots, N$

* Generate $q^{(i)} \sim \text{Uniform}[0,1]$

* Generate $D^{*(i)} \sim w_{js}$

**Accept/Reject Criterion:**

* If $d(D_{obs}, D^{*(i)}) < \epsilon$, accept $q^{(i)}$, else reject.

**Posterior Approximation:**

* The accepted parameter values approximate the posterior distribution $P(q|D_{obs})$.

#### Function to Generate Posterior Samples

Next, we generate posterior samples by defining a function that takes the pair of observed data (given_data1, given_data2), a similarity measure (distance), a prior distribution for the parameter, a data-simulating function, and a tolerance level. This function returns a posterior sample of the parameter $q$ that satisfies the tolerance condition for the distance between simulated and given data. 

We also provide flexibility by allowing the user to define the `prior_distribution` function to return either a named list (with specific keys: q_c1, q_h1, q_c2, q_h2) or an unnamed vector of length 4, ensuring seamless handling of both formats. The returned output $q$ is a list or a vector, depending on what the user feeds as `prior_distribution`. By default, if a vector is fed, we choose the ordering to be q_c1, q_h1, q_c2, q_h2.

```{r}
generate_abc_sample <- function(given_data1, given_data2,
                                distance,
                                prior_distribution,
                                data_simulating_function,
                                epsilon) {
    
    #Throw error if tolerance level is not positive
    if(epsilon <= 0){
      stop("Epsilon must be positive")
    }
        
    while(TRUE) {
        q <- prior_distribution()
        
        # Throw error if prior_distribution is not of the right shape
        if(length(q)!=4){
          stop("Prior_distribution must return either a list or a vector of length 4")
        }
      
        # Handle both list and vector formats for q
        if (is.list(q)) {
            # Check that list elements are named correctly
            required_names <- c("q_c1", "q_c2", "q_h1", "q_h2")
            if (!setequal(required_names, names(q))) {
                stop("Prior_distribution list must have names: q_c1, q_h1, q_c2, q_h2")
            }
            # Access elements by name
            q_c1 <- q$q_c1
            q_h1 <- q$q_h1
            q_c2 <- q$q_c2
            q_h2 <- q$q_h2
        } else if (is.vector(q)) {
            # Access elements by index
            # Default ordering if given a vector (q_c1, q_h1, q_c2, q_h2)
            q_c1 <- q[1]
            q_h1 <- q[2]
            q_c2 <- q[3]
            q_h2 <- q[4]
        } else {
            stop("Prior_distribution must return either a list or a vector")
        }
        
        household_size1 <- ncol(given_data1)
        n_households1 <- colSums(given_data1)
        
        simulated_data1 <- data_simulating_function(q_c1, q_h1, household_size1, n_households1)
        
        household_size2 <- ncol(given_data2)
        n_households2 <- colSums(given_data2)
        
        simulated_data2 <- data_simulating_function(q_c2, q_h2, household_size2, n_households2)
        
        if(distance(given_data1, given_data2, simulated_data1, simulated_data2) < epsilon) {
            return(q)
        }
    }
}
```

### Influenza A outbreaks in Tecumseh, Michigan

The following code stores the observed data from Table 2 of the Supplementary Material. 

- given_data1: Influenza A (H3N2) infection in 1977-78, Tecumseh, Michigan. [3]
- given_data2: Influenza A (H3N2) infection in 1980-81, Tecumseh, Michigan. [3]  

```{r}
given_data1 <- matrix(c(66, 87, 25, 22, 4,
                        13, 14, 15, 9, 4,
                        0, 4, 4, 9, 1,
                        0, 0, 4, 3, 1,
                        0, 0, 0, 1, 1, 
                        0, 0, 0, 0, 0), byrow = TRUE, ncol=5)

given_data2 <- matrix(c(44, 62, 47, 38, 9,
                        10, 13, 8, 11, 5,
                        0, 9, 2, 7, 3,
                        0, 0, 3, 5, 1,
                        0, 0, 0, 1, 0, 
                        0, 0, 0, 0, 1), byrow = TRUE, ncol=5)

```


#### Note about tolerance level $\epsilon$ 

Before applying ABC to get sample parameters, we make a small note about what is considered a reasonable tolerance level. Our datasets represent counts of households with $s$ susceptible and $j$ infected individuals, structured as a 6×5 matrix. **By design, these counts are always integers**. Notably, 10 entries in each dataset are fixed to zero, corresponding to cases where $j>s$. The remaining 20 entries are randomly generated using the simulate_household_data function.

Since both observed data and simulated data are 6×5 matrices with integer entries, and 10 of the entries are always zero, the (unnormalized) Frobenius distance can be sensitive to small differences. For instance, if 5 of the 20 entries are identical and remaining 15 differ by only 1, the Frobenius distance becomes 15. This scale justifies setting $\epsilon = 15$ as a reasonable threshold, which is what we use to generate the posteriors. This choice also falls in the tolerance schedule used by Toni and Stumpf.

Now we are ready to apply our ABC sample generating function to get posterior samples for $(q_{c1}, q_{h1}, q_{c2}, q_{h2})$.

```{r}
prior_distribution_vector <- function() runif(n = 4, min = 0, max = 1)     

# Generating one posterior sample as a vector, as prior_distribution_vector returns a vector

generate_abc_sample(given_data1, given_data2, avg_frob_distance,
                        prior_distribution_vector, simulate_household_data, epsilon = 20)
```

```{r}
prior_distribution_list <- function() {
  parameters <- runif(n = 4, min = 0, max = 1)
  names(parameters) <- c("q_c1", "q_h1", "q_c2", "q_h2")
  as.list(parameters)
}  

# Generating one posterior sample as a list, as prior_distribution_list returns a list

generate_abc_sample(given_data1, given_data2, avg_frob_distance,
                        prior_distribution_list, simulate_household_data, epsilon = 20)
```

The code below produces 100 parameter samples that are accepted by the algorithm with $\epsilon = 15$. This is a computationally expensive process and took about 2 hours to run on RStudio (local machine).

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take hours. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation, eval=FALSE}
# Generating 100 posterior samples
posterior_samples_Table2 <- replicate(n = 100,
    generate_abc_sample(given_data1, given_data2, avg_frob_distance,
                        prior_distribution, simulate_household_data, epsilon = 15))
```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save posterior_samples_Table2.rds on your local machine before running this
# saveRDS(posterior_samples_Table2, "posterior_samples_Table2.rds")
posterior_samples_Table2 <- readRDS("posterior_samples_Table2.rds")
```

Here are a few posterior samples from the 100 generated above.

```{r}
posterior_samples_Table2[, 95:100]
```

We now extract individual parameters $q_{c1}$, $q_{h1}$, $q_{c2}$ and $q_{h2}$ from the samples. Note that since `prior_distribution` returned a named list of parameters, `posterior_samples_Table2` is a matrix with row names ("q_c1", "q_h1", "q_c2", "q_h2"). 

```{r}
is.matrix(posterior_samples_Table2)
rownames(posterior_samples_Table2)
```

However, each row of the matrix `posterior_samples_Table2` is a list.

```{r}
typeof(posterior_samples_Table2["q_c1", ])
```

Therefore, we use the function `unlist` to store individual posterior parameter values as vectors in order to plot them.

```{r}
# Extracting samples of parameters q_c1, q_h1, q_c2, q_h2 from the posterior samples
q_c1 <- unlist(posterior_samples_Table2["q_c1", ])
q_h1 <- unlist(posterior_samples_Table2["q_h1", ])
q_c2 <- unlist(posterior_samples_Table2["q_c2", ])
q_h2 <- unlist(posterior_samples_Table2["q_h2", ])
```

Finally, we plot the simulated parameters $q_h$ vs $q_c$ for the two outbreaks. This recreates Figure 3(a) from [Toni and Stumpf (2010)](https://watermark.silverchair.com/bioinformatics_26_1_104.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA3swggN3BgkqhkiG9w0BBwagggNoMIIDZAIBADCCA10GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMjV15kHzFknWstbVZAgEQgIIDLvGZcFAgMxJ2FtxTGrsPvkAoO-imsFvwyY2RJRbPWpz_WYOR3ZYIIXnJpmCp_pOOlhB9fGwvPCCBNkFN7jjQvo-jtSs3vYGT9U9ABO1ngcGxGq0M_-xfz5QDNcfLJpdHphXjvXPmNQKw-FbmW7Z-lM4VADhWgRMXeAa69IcQWbf3O3M4YVlAfOhNibTRLt8QLpayutZlbZAwX6aC2a13wmjnKF6Vx3WJWazbewssqJov9CmNXprFKUqnhcq1QLZ4oaGSKYaxVFpmwB2ZylzUBbliQ3fYN6VRAfleLXrmyvOymid2GtXNnhrslyx6SN2OSgbXU0YIgfSgCk5OaCETsFY7VMGzLCuUTB776n6hDJKcZ-Hb7RelIJxLeOZteaxRVOiu-a9pG5NbQQuueQtS0C-kqHlVksEwUAucqzS9UXX3ucvmsIgYK-jQQ8jmtqPTjVkdFGhR1J3LzOw7VJCJQy4b_a_WZLDNS7bskxvvZgU7DOZAVHxYu1aPUHh3UaeJ-5oMwJ-sqFWg_6ZruUPk4L9f1KB1siRgSmxw-Eo4JHKXjSEsIXAylD3m_trgxEIxkeqgXFJ867U-qJxeG39ToS9BptAG_IGK-HfMD0ovPK9mKHXvrp32fRO5S0oiqaCMa8kV4DGwbZjaMArJDV9Ps3WNw_EE2E8m7J4UjiqLNQkihUtUM6d4xmJ-S4zo-qPJkr0ajWkDhQwkeJ1wsaYGXItivcoAB4lzyQmG3Zs5kQQIIa2m4hveEf2mDlglHMoPHTAN5hGG-9_LegexhFcKAZTguF4nNpozqAVsIQaj8DeAaHWY8AvjP5HjDTgYHs4ni3w7EjULGDSroFhBndTpCAMNjtY9yIqoh248Bf7ayWtBCXUx1yJyIamAPGeHej3nPnf80TACr2Of6fJicQ-hFcdVGzQj8qiq8b9GuOFFJ43SnZudftdAwwlA0mQb30ZhvgJvsYngGY752-NegVEd2F_r6N2Jkw-G-fzEnoObf6OXzGaYlNSq4_s4vGnQ38HGe_HPk7fdDIESUA35j1SbK5K83274IWvReND0Byubb2MmgZ4fJ90sCCKnjLeu9ho) using the Supplementary data in Table 2. As in the original paper, we use \textcolor{red}{red} for the 1977 outbreak ($q_{h1}, q_{c1}$) and \textcolor{blue}{blue} for the 1980 outbreak ($q_{h2}, q_{c2}$).


```{r}
# Recreate Figure 3(a) with n = 100 posterior samples and epsilon = 15

par(pty = "s")      
# Add first set of data (q_h1, q_c1) in red
plot(q_h1, q_c1, 
     main = "q_h vs q_c (n = 100, epsilon = 15)", 
     xlab = "q_h", ylab = "q_c", 
     xlim = c(0, 1), ylim = c(0, 1), 
     pch = 16, col = "red", 
     cex = 1.5)  

# Add second set of data (q_h2, q_c2) in blue
points(q_h2, q_c2, pch = 16, col = "blue", cex = 1.2)

# Add a legend
legend("bottomleft", 
       legend = c("given_data1 from Table 2", "given_data2 from Table 2"), 
       col = c("red", "blue"), 
       pch = 16, bty = "n")
```

Mean values of posterior samples are given below.

```{r}
mean_q_c1 <- mean(q_c1)
mean_q_h1 <- mean(q_h1)
mean_q_c2 <- mean(q_c2)
mean_q_h2 <- mean(q_h2)

cat("Mean of q_c1:", mean_q_c1, "\n")
cat("Mean of q_h1:", mean_q_h1, "\n")
cat("Mean of q_c2:", mean_q_c2, "\n")
cat("Mean of q_h2:", mean_q_h2, "\n")
```


### Influenza A and B outbreaks in Seattle, Washington

The following code stores the observed data from Table 3 of the Supplementary Material. 

- given_data3: Influenza B infection in 1975-76, Seattle, Washington. [4]
- given_data4: Influenza A (H1N1) infection in 1978-79, Seattle, Washington. [4]  

```{r}
given_data3 <- matrix(c(9, 12, 18, 9, 4,
                        1, 6, 6, 4, 3,
                        0, 2, 3, 4, 0,
                        0, 0, 1, 3, 2,
                        0, 0, 0, 0, 0, 
                        0, 0, 0, 0, 0), byrow = TRUE, ncol=5)

given_data4 <- matrix(c(15, 12, 4,
                        11, 17, 4,
                        0, 21, 4,
                        0, 0, 5 ), byrow = TRUE, ncol=3)

```


As with data from Table 2, we now apply our sample generating function to get posterior samples for $(q_{c3}, q_{h3}, q_{c4}, q_{h4})$. Since the given data in Table 3 is smaller than the one in Table 2, we are able to choose a smaller $\epsilon$ and still be within computational limits. We use $\epsilon = 8$ as a reasonable tolerance level. This value also falls in the tolerance schedule used by Toni and Stumpf.

```{r}
# Generating one posterior sample 

generate_abc_sample(given_data3, given_data4, avg_frob_distance, 
                    prior_distribution, simulate_household_data, epsilon = 8)
```

The code below produces 200 parameter samples that are accepted by the algorithm with $\epsilon = 8$.

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation-2, eval=FALSE}
# Generating 200 posterior samples
posterior_samples_Table3 <- replicate(n = 200,
    generate_abc_sample(given_data3, given_data4, avg_frob_distance, 
                        prior_distribution, simulate_household_data, epsilon = 8))
```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save posterior_samples_Table3.rds on your local machine before running this
# saveRDS(posterior_samples_Table3, "posterior_samples_Table3.rds")
posterior_samples_Table3 <- readRDS("posterior_samples_Table3.rds")
```

Here are a few posterior samples from the 200 generated above.

```{r}
posterior_samples_Table3[, 195:200]
```

We now extract individual parameters $q_{c1}$, $q_{h1}$, $q_{c2}$ and $q_{h2}$ from the samples. 

```{r}
# Extracting samples of parameters q_c1, q_h1, q_c2, q_h2 from the posterior samples
q_c1 <- unlist(posterior_samples_Table3["q_c1", ])
q_h1 <- unlist(posterior_samples_Table3["q_h1", ])
q_c2 <- unlist(posterior_samples_Table3["q_c2", ])
q_h2 <- unlist(posterior_samples_Table3["q_h2", ])
```

Finally, we plot the simulated parameters $q_h$ vs $q_c$ for the two outbreaks. This recreates Figure 3(b) from [Toni and Stumpf (2010)](https://watermark.silverchair.com/bioinformatics_26_1_104.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA3swggN3BgkqhkiG9w0BBwagggNoMIIDZAIBADCCA10GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMjV15kHzFknWstbVZAgEQgIIDLvGZcFAgMxJ2FtxTGrsPvkAoO-imsFvwyY2RJRbPWpz_WYOR3ZYIIXnJpmCp_pOOlhB9fGwvPCCBNkFN7jjQvo-jtSs3vYGT9U9ABO1ngcGxGq0M_-xfz5QDNcfLJpdHphXjvXPmNQKw-FbmW7Z-lM4VADhWgRMXeAa69IcQWbf3O3M4YVlAfOhNibTRLt8QLpayutZlbZAwX6aC2a13wmjnKF6Vx3WJWazbewssqJov9CmNXprFKUqnhcq1QLZ4oaGSKYaxVFpmwB2ZylzUBbliQ3fYN6VRAfleLXrmyvOymid2GtXNnhrslyx6SN2OSgbXU0YIgfSgCk5OaCETsFY7VMGzLCuUTB776n6hDJKcZ-Hb7RelIJxLeOZteaxRVOiu-a9pG5NbQQuueQtS0C-kqHlVksEwUAucqzS9UXX3ucvmsIgYK-jQQ8jmtqPTjVkdFGhR1J3LzOw7VJCJQy4b_a_WZLDNS7bskxvvZgU7DOZAVHxYu1aPUHh3UaeJ-5oMwJ-sqFWg_6ZruUPk4L9f1KB1siRgSmxw-Eo4JHKXjSEsIXAylD3m_trgxEIxkeqgXFJ867U-qJxeG39ToS9BptAG_IGK-HfMD0ovPK9mKHXvrp32fRO5S0oiqaCMa8kV4DGwbZjaMArJDV9Ps3WNw_EE2E8m7J4UjiqLNQkihUtUM6d4xmJ-S4zo-qPJkr0ajWkDhQwkeJ1wsaYGXItivcoAB4lzyQmG3Zs5kQQIIa2m4hveEf2mDlglHMoPHTAN5hGG-9_LegexhFcKAZTguF4nNpozqAVsIQaj8DeAaHWY8AvjP5HjDTgYHs4ni3w7EjULGDSroFhBndTpCAMNjtY9yIqoh248Bf7ayWtBCXUx1yJyIamAPGeHej3nPnf80TACr2Of6fJicQ-hFcdVGzQj8qiq8b9GuOFFJ43SnZudftdAwwlA0mQb30ZhvgJvsYngGY752-NegVEd2F_r6N2Jkw-G-fzEnoObf6OXzGaYlNSq4_s4vGnQ38HGe_HPk7fdDIESUA35j1SbK5K83274IWvReND0Byubb2MmgZ4fJ90sCCKnjLeu9ho) using the Supplementary data in Table 3. As in the original paper, we use \textcolor{red}{red} for the 1975 outbreak ($q_{h3}, q_{c3}$) and \textcolor{blue}{blue} for the 1978 outbreak ($q_{h4}, q_{c4}$).


```{r}
# Recreate Figure 3(b) with n = 200 posterior samples and epsilon = 8

par(pty = "s")      
# Add first set of data (q_h1, q_c1) in red
plot(q_h1, q_c1, 
     main = "q_h vs q_c (n = 200, epsilon = 8)", 
     xlab = "q_h", ylab = "q_c", 
     xlim = c(0, 1), ylim = c(0, 1), 
     pch = 16, col = "red", 
     cex = 1.5)  

# Add second set of data (q_h2, q_c2) in blue
points(q_h2, q_c2, pch = 16, col = "blue", cex = 1.2)

# Add a legend
legend("bottomleft", 
       legend = c("given_data3 from Table 3", "given_data4 from Table 3"), 
       col = c("red", "blue"), 
       pch = 16, bty = "n")
```

Mean values of posterior samples are given below.

```{r}
mean_q_c1 <- mean(q_c1)
mean_q_h1 <- mean(q_h1)
mean_q_c2 <- mean(q_c2)
mean_q_h2 <- mean(q_h2)

cat("Mean of q_c1:", mean_q_c1, "\n")
cat("Mean of q_h1:", mean_q_h1, "\n")
cat("Mean of q_c2:", mean_q_c2, "\n")
cat("Mean of q_h2:", mean_q_h2, "\n")
```

## The 2-parameter model

The plot comparing models of outbreaks of the same strain of the virus suggests that different outbreaks of the same strain of a virus can be modeled using the same parameters. To this end, in this section we write functions for the two parameter model described by Toni and Stumpf and use this model to find parameters for the model of influenza A. This process follows the process for the four parameter model closely. We can again reuse the functions `W_js_matrix` and `simulate_household_data` without any changes. Below we rework the functions `prior_distribution`, and `generate_abc_samples` to work with the 2-parameter model.

```{r}
# Returns 2 random values from the uniform prior distribution on [0,1]
prior_distribution_2 <- function() {
  parameters <- runif(n = 2, min = 0, max = 1)
  names(parameters) <- c("q_c", "q_h")
  as.list(parameters)
}
```

To generate posterior samples, we use two approaches:

- Generating two datasets using the same parameters $q_c, q_h$ and computing their average Frobenius distance with the given datasets

- Combining the given data into a single dataset, generating one dataset using $q_c,q_h$ and computing their Frobenius distance.

We believe that the authors employed the former approach.

### 2-parameter model with separate datasets for influenza A outbreaks

The following function generates posteriors for the 2 parameter model using two separate datasets for each outbreak of the virus.

```{r}
generate_abc_sample_2 <- function(given_data1, given_data2,
                                distance,
                                prior_distribution,
                                data_simulating_function,
                                epsilon) {
    
    #Throw error if tolerance level is not positive
    if(epsilon <= 0){
      stop("Epsilon must be positive")
    }
        
    while(TRUE) {
        q <- prior_distribution()
        
        # Throw error if prior_distribution is not of the right shape
        if(length(q)!=2){
          stop("Prior_distribution must return either a list or a vector of length 2")
        }
      
        # Handle both list and vector formats for q
        if (is.list(q)) {
            # Check that list elements are named correctly
            required_names <- c("q_c", "q_h")
            if (!setequal(required_names, names(q))) {
                stop("Prior_distribution list must have names: q_c, q_h")
            }
            # Access elements by name
            q_c <- q$q_c
            q_h <- q$q_h
            
        } else if (is.vector(q)) {
            # Access elements by index
            # Default ordering if given a vector (q_c, q_h)
            q_c <- q[1]
            q_h <- q[2]
            
        } else {
            stop("Prior_distribution must return either a list or a vector")
        }
        
        household_size1 <- ncol(given_data1)
        n_households1 <- colSums(given_data1)
        
        simulated_data1 <- data_simulating_function(q_c, q_h, household_size1, n_households1)
        
        household_size2 <- ncol(given_data2)
        n_households2 <- colSums(given_data2)
        
        simulated_data2 <- data_simulating_function(q_c, q_h, household_size2, n_households2)
        
        if(distance(given_data1, given_data2, simulated_data1, simulated_data2) < epsilon) {
            return(q)
        }
    }
}
```

Now with these modified functions we are able to build a 2 parameter model for influenza A infection in Tecumseh, Michigan [3]. We use the same data again as given in Table 2.

We generate 100 posterior samples using $\epsilon = 20$. 

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation-4, eval=FALSE}
# Generating 100 posterior samples
influenzaA_posterior_samples <- replicate(n = 100,
    generate_abc_sample_2(given_data1, given_data2, avg_frob_distance, 
                        prior_distribution_2, simulate_household_data, epsilon = 20))
```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save influenzaA_posterior_samples.rds on your local machine before running this
# saveRDS(influenzaA_posterior_samples, "influenzaA_posterior_samples.rds")
influenzaA_posterior_samples <- readRDS("influenzaA_posterior_samples.rds")
```

Here are a few posterior samples from the 100 generated above.

```{r}
influenzaA_posterior_samples[, 95:100]
```

We now extract individual parameters $q_{c}$ and $q_{h}$ from the samples. 

```{r}
# Extracting samples of parameters q_c, q_h from the posterior samples
q_c <- unlist(influenzaA_posterior_samples["q_c", ])
q_h <- unlist(influenzaA_posterior_samples["q_h", ])
```

We can approximate the distribution of $q_c$ and $q_h$ using by plotting these samples.

```{r}
par(pty = "s")      

plot(q_h, q_c, 
     main = "q_h vs q_c (n = 100, epsilon = 20)", 
     xlab = "q_h", ylab = "q_c", 
     xlim = c(0, 1), ylim = c(0, 1), 
     pch = 16, col = "magenta", 
     cex = 1.5)  

mtext("Using two separate datasets and average Frobenius distance", side = 1, line = 5, cex = 1.2)
```

Mean values of posterior samples are given below.

```{r}
mean_q_c <- mean(q_c)
mean_q_h <- mean(q_h)

cat("Mean of q_c:", mean_q_c, "\n")
cat("Mean of q_h:", mean_q_h, "\n")
```

### 2-parameter model with combined dataset for influenza A outbreaks

In order to use the outbreak datasets as a single combined data, we first need to define the Frobenius distance function, as opposed to the average Frobenius distance function we have been using. 

```{r}
# Uses the Frobenius norm to return the distance between "given_data" and "simulated_data"
frob_distance <- function(given_data, simulated_data){
  # Throw error if given_data and simulated_data aren't the same size
  if(ncol(given_data)!=ncol(simulated_data) || nrow(given_data) != nrow(simulated_data)){
    stop("Invalid inputs: given_data and simulated_data have different dimensions")
  }

  total_distance = frobenius_norm(given_data-simulated_data)
  return(total_distance)
}
```

The following function generates posteriors for the 2 parameter model using a single combined dataset for the two influenza A outbreaks.

```{r}
# This function generates posteriors for the 2 parameter model using one combined dataset
generate_abc_sample_combined <- function(given_data,
                                distance,
                                prior_distribution,
                                data_simulating_function,
                                epsilon) {
    
    #Throw error if tolerance level is not positive
    if(epsilon <= 0){
      stop("Epsilon must be positive")
    }
        
  
    while(TRUE) {
      
        q <- prior_distribution()
        
        # Throw error if prior_distribution is not of the right shape
        if(length(q)!=2){
          stop("Prior_distribution must return either a list or a vector of length 2")
        }
      
        # Handle both list and vector formats for q
        if (is.list(q)) {
            # Check that list elements are named correctly
            required_names <- c("q_c", "q_h")
            if (!setequal(required_names, names(q))) {
                stop("Prior_distribution list must have names: q_c, q_h")
            }
            # Access elements by name
            q_c <- q$q_c
            q_h <- q$q_h
            
        } else if (is.vector(q)) {
            # Access elements by index
            # Default ordering if given a vector (q_c, q_h)
            q_c <- q[1]
            q_h <- q[2]
            
        } else {
            stop("Prior_distribution must return either a list or a vector")
        }
        
        household_size <- ncol(given_data)
        n_households <- colSums(given_data)
        
        simulated_data <- data_simulating_function(q_c, q_h, household_size, n_households)
        
        if(distance(given_data, simulated_data) < epsilon) {
            return(q)
        }
    }
}
```

Now with these modified functions we are able to build another 2-parameter model for influenza A. To do this we will first combine the data given for the 1977-78 and 1980-81 outbreaks of influenza A into one matrix by adding the matrices for the separate outbreaks. 

```{r}
combined_influenza_data <- given_data1 + given_data2
```

Using this data set, we again generate 100 posterior samples using $\epsilon = 20$. 

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation-3, eval=FALSE}
combined_influenza_posterior_samples <- replicate(n = 100, generate_abc_sample_combined(combined_influenza_data, frob_distance,
                      prior_distribution_2, simulate_household_data, epsilon = 20))
```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save combined_influenza_posterior_samples.rds on your local machine before running this
# saveRDS(combined_influenza_posterior_samples, "combined_influenza_posterior_samples.rds")
combined_influenza_posterior_samples <- readRDS("combined_influenza_posterior_samples.rds")
```

Here are a few posterior samples from the 100 generated above.

```{r}
combined_influenza_posterior_samples[, 95:100]
```

We now extract individual parameters $q_{c}$ and $q_{h}$ from the samples. 

```{r}
# Extracting samples of parameters q_c, q_h from the posterior samples
q_c <- unlist(combined_influenza_posterior_samples["q_c", ])
q_h <- unlist(combined_influenza_posterior_samples["q_h", ])
```

We can approximate the distribution of $q_c$ and $q_h$ using by plotting these samples.

```{r}
par(pty = "s")      

plot(q_h, q_c, 
     main = "q_h vs q_c (n = 100, epsilon = 20)", 
     xlab = "q_h", ylab = "q_c", 
     xlim = c(0, 1), ylim = c(0, 1), 
     pch = 16, col = "orange", 
     cex = 1.5)  

mtext("Using one combined dataset and Frobenius distance", side = 1, line = 5, cex = 1.2)
```

Mean values of posterior samples are given below.

```{r}
mean_q_c <- mean(q_c)
mean_q_h <- mean(q_h)

cat("Mean of q_c:", mean_q_c, "\n")
cat("Mean of q_h:", mean_q_h, "\n")
```


## The 3-Parameter model

The plot comparing models of the outbreaks of different strains of the virus suggests that outbreaks of different strains have the same $q_h$ but different $q_c$. This can be modeled using a 3-parameter model same parameters $(q_{c1},q_{c2},q_h)$. We can again reuse the functions `W_js_matrix`, `simulate_household_data` and `av_frob_distance` without any changes. Below we rework the functions `prior_distribution`, and `generate_abc_samples` to work with the three parameter model.


```{r}
# Returns 3 random values from the uniform prior distribution on [0,1]
prior_distribution_3 <- function() {
  parameters <- runif(n = 3, min = 0, max = 1)
  names(parameters) <- c("q_c1", "q_c2", "q_h")
  as.list(parameters)
}
```


```{r}
# This function generates posteriors for the 3 parameter model
generate_abc_sample_3 <- function(given_data1, given_data2,
                                distance,
                                prior_distribution,
                                data_simulating_function,
                                epsilon) {
    
    #Throw error if tolerance level is not positive
    if(epsilon <= 0){
      stop("Epsilon must be positive")
    }
        
    while(TRUE) {
        q <- prior_distribution()
        
        # Throw error if prior_distribution is not of the right shape
        if(length(q)!=3){
          stop("Prior_distribution must return either a list or a vector of length 3")
        }
      
        # Handle both list and vector formats for q
        if (is.list(q)) {
            # Check that list elements are named correctly
            required_names <- c("q_c1", "q_c2", "q_h")
            if (!setequal(required_names, names(q))) {
                stop("Prior_distribution list must have names: q_c1, q_c2, q_h")
            }
            # Access elements by name
            q_c1 <- q$q_c1
            q_c2 <- q$q_c2
            q_h <- q$q_h
        } else if (is.vector(q)) {
            # Access elements by index
            # Default ordering if given a vector (q_c1, q_c2, q_h)
            q_c1 <- q[1]
            q_c2 <- q[2]
            q_h <- q[3]
        } else {
            stop("Prior_distribution must return either a list or a vector")
        }
        
        
        household_size1 <- ncol(given_data1)
        n_households1 <- colSums(given_data1)
        
        simulated_data1 <- data_simulating_function(q_c1, q_h, household_size1, n_households1)
        
        household_size2 <- ncol(given_data2)
        n_households2 <- colSums(given_data2)
        
        simulated_data2 <- data_simulating_function(q_c2, q_h, household_size2, n_households2)
        
        if(distance(given_data1, given_data2, simulated_data1, simulated_data2) < epsilon) {
            return(q)
        }
    }
}
```

### 3 Parameter Model for Influenza A and Influenz B Outbreaks

Now with these modified functions we are able to build a 3 parameter model for influenza A and influenza B. We use the same data again as given in Table 3.

Here we again use $\epsilon = 8$ for this smaller dataset and generate 200 samples.

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation-5, eval=FALSE}
# Generating 200 posterior samples
influenzaAB_posterior_samples <- replicate(n = 200,
    generate_abc_sample_3(given_data3, given_data4, avg_frob_distance, 
                        prior_distribution_3, simulate_household_data, epsilon = 8))
```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save influenzaAB_posterior_samples on your local machine before running this
# saveRDS(influenzaAB_posterior_samples, "influenzaAB_posterior_samples.rds")
influenzaAB_posterior_samples <- readRDS("influenzaAB_posterior_samples.rds")
```

Here are a few posterior samples from the 200 generated above.

```{r}
influenzaAB_posterior_samples[, 195:200]
```

We now extract individual parameters $q_{c1}$, $q_{c2}$ and $q_{h}$ from the samples. 

```{r}
# Extracting samples of parameters q_c1, q_c2, q_h from the posterior samples
q_c1 <- unlist(influenzaAB_posterior_samples["q_c1", ])
q_c2 <- unlist(influenzaAB_posterior_samples["q_c2", ])
q_h <- unlist(influenzaAB_posterior_samples["q_h", ])
```

We can represent all of this data in a scatterplot similar to the ones above.

```{r}
par(pty = "s")      

# Add first set of data (q_h, q_c1) in red
plot(q_h, q_c1, 
     main = "q_h vs q_c (n = 200, epsilon = 8)", 
     xlab = "q_h", ylab = "q_c", 
     xlim = c(0, 1), ylim = c(0, 1), 
     pch = 16, col = "red", 
     cex = 1.5)  

# Add second set of data (q_h, q_c2) in blue
points(q_h, q_c2, pch = 16, col = "blue", cex = 1.2)

# Add a legend
legend("bottomleft", 
       legend = c("given_data3 from Table 3", "given_data4 from Table 3"), 
       col = c("red", "blue"), 
       pch = 16, bty = "n")
```

Mean values of posterior samples are given below.

```{r}
mean_q_c1 <- mean(q_c1)
mean_q_c2 <- mean(q_c2)
mean_q_h <- mean(q_h)

cat("Mean of q_c1:", mean_q_c1, "\n")
cat("Mean of q_c2:", mean_q_c2, "\n")
cat("Mean of q_h:", mean_q_h, "\n")
```

## Model Comparison using ABC rejection sampler

We can also use ABC to compare different models. Below is a brief algorithm showing how to do the same:

**Inputs:**

* A set of models to compare: $m^{(i)} \in M$

* Prior over models $P(m)$

* Prior over parameters for every model $P(q | m)$

* Data simulator: $f(q|m) = D^*(q)$

* Tolerance $\epsilon$

**Simulations:** for $i = 1, 2, \dots, N$

* Sample model $m^{(i)} \sim P(m)$

* Sample parameters $q^{(i)} \sim P(q|m^{(i)})$

* Generate $D^{*(i)} \sim f(q^{(i)} | m^{(i)})$

**Accept/Reject Criterion:**

* If $d(D_{obs}, D^{*(i)}) < \epsilon$, accept $q^{(i)}$, else reject.

**Model Posterior Probability Approximation:**

* The ratio of number of accepted parameter values approximate the model posterior distribution $P(m^{(i)}|D_{obs})$.

$$P(m^{(i)}|D_{obs}) \approx \cfrac{\text{# accepted samples for } m^{(i)}}{\text{# total accepted samples}}$$

That is, the more often a sample belonging to a model $m^{(i)}$ is accepted, the higher the evidence for this model.

### 2-Parameter Model vs 4-Parameter Model for different outbreaks of the same virus

Below we write a function to compare the 2-parameter model (model 1) and the 4-parameter model (model 2) for the two outbreaks of influenza A strain. Prior over models is uniform over model 1 and model 2, prior for parameters is Uniform[0,1] for both models and data generating function is the same as before, i.e. `simulate_household_data`. For the 2-parameter model, we don't combine the datasets for the two outbreaks and take the average Frobenius distance to replicate the authors' results in Figure 1(a) of Supplementary data. 

We use a subset of the tolerance schedule used by the authors, namely $\epsilon = \{100, 80, 50, 30, 20\}$.

```{r}
# Function to calculate posterior model probabilities 
# for model 1 (2-parameter) and model 2 (4-parameter) for Table 2
model_posterior_2vs4 <- function(given_data1, given_data2, epsilon, n_samples) {
  
  # Initialize counters for accepted samples
  accepted_samples_model_1 <- 0
  accepted_samples_model_2 <- 0
  
  for (i in 1:n_samples) {
    
    # Run the 2-parameter model (model 1)
    # We don't combine the datasets for the two outbreaks in this section
    
    q_model_1 <- prior_distribution_2()
    
    q_c <- q_model_1$q_c
    q_h <- q_model_1$q_h
     
       
    household_size11 <- ncol(given_data1)
    n_households11 <- colSums(given_data1)
        
    simulated_data11 <- simulate_household_data(q_c, q_h, household_size11, n_households11)
        
    household_size12 <- ncol(given_data2)
    n_households12 <- colSums(given_data2)
        
    simulated_data12 <- simulate_household_data(q_c, q_h, household_size12, n_households12)
    
    if (avg_frob_distance(given_data1, given_data2, simulated_data11, simulated_data12) < epsilon) {
      accepted_samples_model_1 <- accepted_samples_model_1 + 1
    }
    
    # Run the 4-parameter model (model 2)
    
    q_model_2 <- prior_distribution()
    
    q_c1 <- q_model_2$q_c1
    q_h1 <- q_model_2$q_h1
    q_c2 <- q_model_2$q_c2
    q_h2 <- q_model_2$q_h2
    
    
    household_size21 <- ncol(given_data1)
    n_households21 <- colSums(given_data1)
        
    simulated_data21 <- simulate_household_data(q_c1, q_h1, household_size21, n_households21)
        
    household_size22 <- ncol(given_data2)
    n_households22 <- colSums(given_data2)
        
    simulated_data22 <- simulate_household_data(q_c2, q_h2, household_size22, n_households22)

    if (avg_frob_distance(given_data1, given_data2, simulated_data21, simulated_data22) < epsilon) {
      accepted_samples_model_2 <- accepted_samples_model_2 + 1
    }
    
  }
  
  
  # Calculate the posterior probabilities for both models
  
  total_accepted_samples <- accepted_samples_model_1 + accepted_samples_model_2
  
  if (total_accepted_samples == 0) {
    stop("No samples were accepted for either model. Adjust epsilon or check your data.")
  }
  
  posterior_prob_model_1 <- accepted_samples_model_1 / total_accepted_samples
  posterior_prob_model_2 <- accepted_samples_model_2 / total_accepted_samples
  
  return(list(model_1_posterior_prob = posterior_prob_model_1,
              model_2_posterior_prob = posterior_prob_model_2,
              accepted_samples_model_1 = accepted_samples_model_1,
              accepted_samples_model_2 = accepted_samples_model_2))
}

```

This function takes the datasets for the two outbreaks, a tolerance level and number of iterations to perform, and returns a list containing posterior probabilities for both models as well as the number of accepted samples for both models.


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save model_postr as rds on your local machine before running this
# saveRDS(model_postr_100, "1model_postr_100.rds")
# saveRDS(model_postr_80, "1model_postr_80.rds")
# saveRDS(model_postr_50, "1model_postr_50.rds")
# saveRDS(model_postr_30, "1model_postr_30.rds")
# saveRDS(model_postr_20, "1model_postr_20.rds")
model_postr_100 <- readRDS("1model_postr_100.rds")
model_postr_80 <- readRDS("1model_postr_80.rds")
model_postr_50 <- readRDS("1model_postr_50.rds")
model_postr_30 <- readRDS("1model_postr_30.rds")
model_postr_20 <- readRDS("1model_postr_20.rds")
```

Here we approximate posterior probabilities for the models using 1000 iterations and tolerance levels from our tolerance schedule.

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation-6, eval=FALSE}
model_postr_100 <- model_posterior_2vs4(given_data1, given_data2, epsilon = 100, n_samples = 1000)

model_postr_80 <- model_posterior_2vs4(given_data1, given_data2, epsilon = 80, n_samples = 1000)

model_postr_50 <- model_posterior_2vs4(given_data1, given_data2, epsilon = 50, n_samples = 1000)

model_postr_30 <- model_posterior_2vs4(given_data1, given_data2, epsilon = 30, n_samples = 1000)

model_postr_20 <- model_posterior_2vs4(given_data1, given_data2, epsilon = 20, n_samples = 1000)
```

Here are the outputs from the above code in a dataframe form for easy readability. We don't display the code for it, but it can be found in our RMD file. 

```{r, echo = FALSE}
# Create a table using cbind
model_table <- cbind(
  "n_samples = 1000" = c(
    "epsilon = 100",
    "epsilon = 80",
    "epsilon = 50",
    "epsilon = 30",
    "epsilon = 20"
  ),
  "Model 1 Posterior Probability" = c(
    round(model_postr_100$model_1_posterior_prob,3),
    round(model_postr_80$model_1_posterior_prob,3),
    round(model_postr_50$model_1_posterior_prob,3),
    round(model_postr_30$model_1_posterior_prob,3),
    round(model_postr_20$model_1_posterior_prob,3)
  ),
  "Model 2 Posterior Probability" = c(
    round(model_postr_100$model_2_posterior_prob,3),
    round(model_postr_80$model_2_posterior_prob,3),
    round(model_postr_50$model_2_posterior_prob,3),
    round(model_postr_30$model_2_posterior_prob,3),
    round(model_postr_20$model_2_posterior_prob,3)
  ),
  "Accepted Samples Model 1" = c(
    round(model_postr_100$accepted_samples_model_1,3),
    round(model_postr_80$accepted_samples_model_1,3),
    round(model_postr_50$accepted_samples_model_1,3),
    round(model_postr_30$accepted_samples_model_1,3),
    round(model_postr_20$accepted_samples_model_1,3)
  ),
  "Accepted Samples Model 2" = c(
    round(model_postr_100$accepted_samples_model_2,3),
    round(model_postr_80$accepted_samples_model_2,3),
    round(model_postr_50$accepted_samples_model_2,3),
    round(model_postr_30$accepted_samples_model_2,3),
    round(model_postr_20$accepted_samples_model_2,3)
  )
)

# Convert the table to a data frame for better readability
model_table <- as.data.frame(model_table)
print(model_table)

```

Finally, we make bar plots for posterior probabilities of both models.


```{r}
par(mfrow = c(2, 3), mar = c(4, 4, 3, 2))

# Define prior probabilities of both models as 0.5
prior <- c(0.5, 0.5)

# Plot prior
barplot(
  prior,
  names.arg = c("Model 1", "Model 2"),
  ylim = c(0, 1),
  col = "beige",
  main = "Prior",
  ylab = "Probability"
)

# Define posterior probabilities
model_postr <- list(
  "100" = c(model_postr_100$model_1_posterior_prob, model_postr_100$model_2_posterior_prob),
  "80" = c(model_postr_80$model_1_posterior_prob, model_postr_80$model_2_posterior_prob),
  "50" = c(model_postr_50$model_1_posterior_prob, model_postr_50$model_2_posterior_prob),
  "30" = c(model_postr_30$model_1_posterior_prob, model_postr_30$model_2_posterior_prob),
  "20" = c(model_postr_20$model_1_posterior_prob, model_postr_20$model_2_posterior_prob)
)

# Plot posterior distributions
for (epsilon in names(model_postr)) {
  barplot(
    model_postr[[epsilon]],
    names.arg = c("Model 1", "Model 2"),
    ylim = c(0, 1),
    col = "beige",
    main = paste("Epsilon =", epsilon),
    ylab = "Posterior Probability"
  )
}

```


### 3-Parameter vs 4-Parameter models for different strain outbreaks

Similarly, we write a function to compare the 3-parameter model (model 1) and the 4-parameter model (model 2) for the outbreaks of influenza A and influenza B strains. This section replicates the findings of Figure 1(b) from the Supplementary data.

We again use a subset of the tolerance schedule used by the authors, namely $\epsilon = \{40, 20, 15, 10, 8\}$.


```{r}
# Function to calculate posterior model probabilities 
# for model 1 (3-parameter) and model 2 (4-parameter) for Table 3
model_posterior_3vs4 <- function(given_data1, given_data2, epsilon, n_samples) {
  
  # Initialize counters for accepted samples
  accepted_samples_model_1 <- 0
  accepted_samples_model_2 <- 0
  
  for (i in 1:n_samples) {
    
    # Run the 3-parameter model (model 1)
    
    q_model_1 <- prior_distribution_3()
    
    q_c11 <- q_model_1$q_c1
    q_c12 <- q_model_1$q_c2
    q_h <- q_model_1$q_h
        
    household_size11 <- ncol(given_data1)
    n_households11 <- colSums(given_data1)
        
    simulated_data11 <- simulate_household_data(q_c11, q_h, household_size11, n_households11)
        
    household_size12 <- ncol(given_data2)
    n_households12 <- colSums(given_data2)
        
    simulated_data12 <- simulate_household_data(q_c12, q_h, household_size12, n_households12)
    
    if (avg_frob_distance(given_data1, given_data2, simulated_data11, simulated_data12) < epsilon) {
      accepted_samples_model_1 <- accepted_samples_model_1 + 1
    }
    
    # Run the 4-parameter model (model 2)
    
    q_model_2 <- prior_distribution()
    
    q_c21 <- q_model_2$q_c1
    q_h21 <- q_model_2$q_h1
    q_c22 <- q_model_2$q_c2
    q_h22 <- q_model_2$q_h2
    
    
    household_size21 <- ncol(given_data1)
    n_households21 <- colSums(given_data1)
        
    simulated_data21 <- simulate_household_data(q_c21, q_h21, household_size21, n_households21)
        
    household_size22 <- ncol(given_data2)
    n_households22 <- colSums(given_data2)
        
    simulated_data22 <- simulate_household_data(q_c22, q_h22, household_size22, n_households22)

    
    if (avg_frob_distance(given_data1, given_data2, simulated_data21, simulated_data22) < epsilon) {
      accepted_samples_model_2 <- accepted_samples_model_2 + 1
    }
    
  }
  
  
  # Calculate the posterior probabilities for both models
  
  total_accepted_samples <- accepted_samples_model_1 + accepted_samples_model_2
  
  if (total_accepted_samples == 0) {
    stop("No samples were accepted for either model. Adjust epsilon or check your data.")
  }
  
  posterior_prob_model_1 <- accepted_samples_model_1 / total_accepted_samples
  posterior_prob_model_2 <- accepted_samples_model_2 / total_accepted_samples
  
  return(list(model_1_posterior_prob = posterior_prob_model_1,
              model_2_posterior_prob = posterior_prob_model_2,
              accepted_samples_model_1 = accepted_samples_model_1,
              accepted_samples_model_2 = accepted_samples_model_2))
}

```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save Model_postr as rds on your local machine before running this
# saveRDS(Model_postr_40, "2Model_postr_40.rds")
# saveRDS(Model_postr_20, "2Model_postr_20.rds")
# saveRDS(Model_postr_15, "2Model_postr_15.rds")
# saveRDS(Model_postr_10, "2Model_postr_10.rds")
# saveRDS(Model_postr_8, "2Model_postr_8.rds")
Model_postr_40 <- readRDS("2Model_postr_40.rds")
Model_postr_20 <- readRDS("2Model_postr_20.rds")
Model_postr_15 <- readRDS("2Model_postr_15.rds")
Model_postr_10 <- readRDS("2Model_postr_10.rds")
Model_postr_8 <- readRDS("2Model_postr_8.rds")
```

Here we approximate posterior probabilities for the models using 1000 iterations and tolerance levels from our tolerance schedule.


```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation-7, eval=FALSE}
Model_postr_40 <- model_posterior_3vs4(given_data3, given_data4, epsilon = 40, n_samples = 1000)

Model_postr_20 <- model_posterior_3vs4(given_data3, given_data4, epsilon = 20, n_samples = 1000)

Model_postr_15 <- model_posterior_3vs4(given_data3, given_data4, epsilon = 15, n_samples = 1000)

Model_postr_10 <- model_posterior_3vs4(given_data3, given_data4, epsilon = 10, n_samples = 1000)

Model_postr_8 <- model_posterior_3vs4(given_data3, given_data4, epsilon = 8, n_samples = 1000)
```

Here are the outputs from the above code in a dataframe form for easy readability. We don't display the code for it, but it can be found in our RMD file. 


```{r, echo = FALSE}
# Create a table using cbind
model_table2 <- cbind(
  "n_samples = 1000" = c(
    "epsilon = 40",
    "epsilon = 20",
    "epsilon = 15",
    "epsilon = 10",
    "epsilon = 8"
  ),
  "Model 1 Posterior Probability" = c(
    round(Model_postr_40$model_1_posterior_prob,3),
    round(Model_postr_20$model_1_posterior_prob,3),
    round(Model_postr_15$model_1_posterior_prob,3),
    round(Model_postr_10$model_1_posterior_prob,3),
    round(Model_postr_8$model_1_posterior_prob,3)
  ),
  "Model 2 Posterior Probability" = c(
    round(Model_postr_40$model_2_posterior_prob,3),
    round(Model_postr_20$model_2_posterior_prob,3),
    round(Model_postr_15$model_2_posterior_prob,3),
    round(Model_postr_10$model_2_posterior_prob,3),
    round(Model_postr_8$model_2_posterior_prob,3)
  ),
  "Accepted Samples Model 1" = c(
    round(Model_postr_40$accepted_samples_model_1,3),
    round(Model_postr_20$accepted_samples_model_1,3),
    round(Model_postr_15$accepted_samples_model_1,3),
    round(Model_postr_10$accepted_samples_model_1,3),
    round(Model_postr_8$accepted_samples_model_1,3)
  ),
  "Accepted Samples Model 2" = c(
    round(Model_postr_40$accepted_samples_model_2,3),
    round(Model_postr_20$accepted_samples_model_2,3),
    round(Model_postr_15$accepted_samples_model_2,3),
    round(Model_postr_10$accepted_samples_model_2,3),
    round(Model_postr_8$accepted_samples_model_2,3)
  )
)

# Convert the table to a data frame for better readability
model_table2 <- as.data.frame(model_table2)
print(model_table2)

```

Finally, we make bar plots for posterior probabilities of both models.

```{r}
par(mfrow = c(2, 3), mar = c(4, 4, 3, 2))

# Define prior probabilities of both models as 0.5
prior <- c(0.5, 0.5)

# Plot prior
barplot(
  prior,
  names.arg = c("Model 1", "Model 2"),
  ylim = c(0, 1),
  col = "beige",
  main = "Prior",
  ylab = "Probability"
)

# Define posterior probabilities
model_postr2 <- list(
  "40" = c(Model_postr_40$model_1_posterior_prob, Model_postr_40$model_2_posterior_prob),
  "20" = c(Model_postr_20$model_1_posterior_prob, Model_postr_20$model_2_posterior_prob),
  "15" = c(Model_postr_15$model_1_posterior_prob, Model_postr_15$model_2_posterior_prob),
  "10" = c(Model_postr_10$model_1_posterior_prob, Model_postr_10$model_2_posterior_prob),
  "8" = c(Model_postr_8$model_1_posterior_prob, Model_postr_8$model_2_posterior_prob)
)

# Plot posterior distributions
for (epsilon in names(model_postr2)) {
  barplot(
    model_postr2[[epsilon]],
    names.arg = c("Model 1", "Model 2"),
    ylim = c(0, 1),
    col = "beige",
    main = paste("Epsilon =", epsilon),
    ylab = "Posterior Probability"
  )
}

```


## Conclusion 

Here we replicate some of the results proposed by Toni and Stumpf using ABC. In particular, we are able to reproduce Figure 3 along with Figure 1 from the Supplementary data. The 4-parameter model proposed in [1] gives a good way of telling whether the ABC procedure gives the same parameters for modeling two different outbreaks. Finally, we use ABC rejection sampler for model comparison to conclude that outbreaks of the same strain of influenza are best modeled by a 2-parameter model and that outbreaks from different strains of influenza are best modeled by a 3-parameter model which accounts for differences in $q_c$ while leaving $q_h$ constant.

## Appendix : Function Tests

### Tests for W_js_matrix

```{r}
# Tests for W_js_matrix

library(testthat)

test_that("W_js_matrix throws error for bad parameters", {
  expect_error(W_js_matrix(0.5, -0.1, 4))        # Negative q_h
  expect_error(W_js_matrix(2, 0.9, 3))           # q_c > 1
  expect_error(W_js_matrix(0.3, 0.2, -7))        # household_size not a positive integer
})

test_that("W_js_matrix works for simple cases", {
  # q_h = q_c = 0 
  W0 <- W_js_matrix(0,0,3)
  expected_W0 <- matrix(c(0,0,0,
                         1,0,0,
                         0,1,0,
                         0,0,1), ncol = 3, byrow = T)
  expect_equivalent(W0, expected_W0)            # expect_equal will fail because of attribute comparison 
                                                # W0 has rownames and colnames, while expected_W0 does not
  # q_h = q_c = 1 
  W1 <- W_js_matrix(1,1,4)
  expected_W1 <- matrix(c(1,1,1,1,
                         0,0,0,0,
                         0,0,0,0,
                         0,0,0,0,
                         0,0,0,0), ncol = 4, byrow = T)
  expect_equivalent(W1, expected_W1)            # W1 has rownames and colnames, while expected_W1 does not
})
  
test_that("W_js_matrix columns sum to 1", {
  for (i in 1:100) {                            # Test for 100 random cases
    q_c <- runif(1, 0, 1)                       # Choose q_c randomly between 0 and 1
    q_h <- runif(1, 0, 1)                       # Choose q_h randomly between 0 and 1
    household_size <- sample(1:10, 1)           # Choose household_size randomly between 1 and 10

    W <- W_js_matrix(q_c, q_h, household_size)
    
    expect_equivalent(colSums(W), rep(1, ncol(W)))   
  }
})
```


### Tests for simulate_household_data

```{r}
# Tests for simulate_household_data

library(testthat)

test_that("simulate_household_data throws error for bad parameters", {
  expect_error(simulate_household_data(0.5, -0.1, 3, c(12,10,6)))   # Negative q_h
  expect_error(simulate_household_data(1.4, 0.8, 4, c(12,10,6,3)))  # q_c > 1
  expect_error(simulate_household_data(0.3, 0.4, 6, c(100,90,8)))   # length(n_households) != household_size 
})

test_that("simulate_household_data works for simple cases", {
  # 0 number of households for household size 3 should return third column 0
  data.0 <- simulate_household_data(0.1, 0.8, 4, c(12,8,0,10))

  expect_equivalent(data.0[,3], rep(0,nrow(data.0)))  
})
  
test_that("simulate_household_data doesn't have more infected than susceptible", {
  q_c <- runif(1, 0, 1)                          # Choose q_c randomly between 0 and 1
  q_h <- runif(1, 0, 1)                          # Choose q_h randomly between 0 and 1
  household_size <- sample(1:10, 1)              # Random household size between 1 and 10
  n_households <- sample(1:100, household_size)  # Random n_households vector of size household_size 
  
  result <- simulate_household_data(q_c, q_h, household_size, n_households)
  
  # Validate that entries for j > s are 0
  for (j in 1 : nrow(result)) {             
    for (s in 1 : ncol(result)) {           
      if (j-1 > s) {                             # j corresponds to row (j-1)
        expect_equivalent(result[j, s], 0)    
      }
    }
  }
})
```

### Tests for Frobenius norm

```{r}
# Tests for frobenius_norm

library(testthat)

test_that("frobenius_norm works on simple cases: zero matrix",{
  # The norm of the 0 matrix should be zero
  zero_matrix <-matrix(c(0,0,0,0,0,0,0,0,0), nrow=3)
  
  expect_equal(frobenius_norm(zero_matrix), 0)
})

test_that("frobenius_norm works on simple cases: c(1,2,0,-2)",{
  # Checks that the Frobenius norm is 3 on this simple case
  example_matrix <-matrix(c(1,2,0,-2), nrow=2)
  
  expect_equal(frobenius_norm(example_matrix), 3)
})

```

### Tests for avg_frob_distance

```{r}
# Tests for avg_frob_distance

library(testthat)

test_that("distance throws error for bad parameters", {
  size1 = matrix(1, nrow=1)
  size2 = matrix(c(2,2,2,2), nrow =2)
  
  expect_error(avg_frob_distance(size1, size1, size2, size1))   # Given_data_1 and simulated_data_1 have different sizes
  expect_error(avg_frob_distance(size1, size1, size1, size2))  # Given_data_2 and simulated_data_2 have different sizes
})

test_that("distance works on simple cases: distance between identical data is 0",{
  data_1 <- matrix(c(1,1,1,1), nrow=2)
  data_2 <- matrix(c(2,2,2,2), nrow=2)
  
  expect_equal(avg_frob_distance(data_1, data_2, data_1, data_2), 0)
})

test_that("distance works on simple cases",{
  given_data_1 <- matrix(c(1,1,1,1), nrow=2)
  given_data_2 <- matrix(c(2,2,2,2), nrow=2)
  simulated_data_both <- matrix(c(0,0,0,0), nrow = 2)
  
  expect_equal(avg_frob_distance(given_data_1, given_data_2, simulated_data_both,  simulated_data_both), 3)
})

```

### Tests for generate_abc_sample

```{r}
# Tests for generate_abc_sample

library(testthat)

test_that("generate_abc_sample throws errors for bad parameters", {
  
  expect_error(generate_abc_sample(given_data3, given_data4, avg_frob_distance, 
                    prior_distribution, simulate_household_data, epsilon = -1)) # negative epsilon
  expect_error(generate_abc_sample(given_data3, given_data4, avg_frob_distance, 
                    prior_distribution, simulate_household_data, epsilon = 0)) # zero epsilon
})

```

```{r}
test_that("generate_abc_sample throws error for invalid prior distribution format", {
  expect_error(generate_abc_sample(given_data3, given_data4, avg_frob_distance, 
                    function() "invalid", simulate_household_data, epsilon = 10)) # Non-list, non-vector prior
  expect_error(generate_abc_sample(given_data3, given_data4, avg_frob_distance, 
                    function() list(), simulate_household_data, epsilon = 10)) # Empty list
})
```

```{r}
test_that("generate_abc_sample throws error for improperly named list elements", {
    # Custom prior_distribution returning an incorrectly named list
    bad_prior_distribution <- function() {
        return(list(q_a1 = 0.5, q_h1 = 0.3, q_c2 = 0.2, q_h2 = 0.7)) # Incorrect names
    }

    expect_error(
        generate_abc_sample(given_data3, given_data4, avg_frob_distance, 
            bad_prior_distribution, simulate_household_data, epsilon = 100
        )
    )
})
```


### Tests for frob_distance

```{r}
# Tests for frob_distance

library(testthat)

test_that("distance_2 throws error for bad parameters", {
  size1 = matrix(1, nrow=1)
  size2 = matrix(c(2,2,2,2), nrow =2)
  
  expect_error(frob_distance(size1, size2))   # Given_data and simulated_data have different sizes
})

test_that("distance works on simple cases: distance between given data and simulated data is 0 when given and simulated data are identical",{
  data_1 <- matrix(c(1,1,1,1), nrow=2)
  
  expect_equivalent(frob_distance(data_1, data_1), 0)
})

test_that("distance works on simple cases",{
  given_data <- matrix(c(1,1,1,1), nrow=2)
  simulated_data <- matrix(c(0,0,0,0), nrow = 2)
  
  expect_equivalent(frob_distance(given_data, simulated_data), 2)
})

```

### Tests for generate_abc_sample_2

```{r}
# Tests for generate_abc_sample_2

library(testthat)

test_that("generate_abc_sample_2 throws errors for bad parameters", {
  
  expect_error(generate_abc_sample_2(given_data1, avg_frob_distance, 
                    prior_distribution_2, simulate_household_data, epsilon = -1)) # negative epsilon
  expect_error(generate_abc_sample_2(given_data1, avg_frob_distance, 
                    prior_distribution_2, simulate_household_data, epsilon = 0)) # zero epsilon
})

```


```{r}
test_that("generate_abc_sample_2 throws error for invalid prior distribution format", {
  expect_error(generate_abc_sample_2(given_data3, given_data4, avg_frob_distance, 
                    function() "invalid", simulate_household_data, epsilon = 10)) # Non-list, non-vector prior
  expect_error(generate_abc_sample_2(given_data3, given_data4, avg_frob_distance, 
                    function() list(), simulate_household_data, epsilon = 10)) # Empty list
})
```

```{r}
test_that("generate_abc_sample_2 throws error for improperly named list elements", {
    # Custom prior_distribution returning an incorrectly named list
    bad_prior_distribution <- function() {
        return(list(q_a = 0.5, q_x = 0.3)) # Incorrect names
    }

    expect_error(
        generate_abc_sample_2(given_data3, given_data4, avg_frob_distance, 
            bad_prior_distribution, simulate_household_data, epsilon = 100
        )
    )
})
```

```{r}
test_that("generate_abc_sample_2 throws error for invalid prior distribution length", {
    # Custom prior_distribution returning an incorrectly named list
    bad_prior_distribution <- function() {
        return(c(0.1, 0.2, 0.3)) # Incorrect length
    }

    expect_error(
        generate_abc_sample_2(given_data3, given_data4, avg_frob_distance, 
            bad_prior_distribution, simulate_household_data, epsilon = 100
        )
    )
})
```

```{r}
test_that("generate_abc_sample_2 handles zero values in prior distribution", {
  result <- generate_abc_sample_2(given_data3, given_data4, avg_frob_distance, 
                                function() c(0, 0), 
                                simulate_household_data, epsilon = 100) 
  expect_equal(result[1], 0)       # Check that the first value is 0
  expect_equal(result[2], 0)       # Check that the second value is 0
})
```


### Tests for generate_abc_sample_3

```{r}
# Tests for generate_abc_sample_3

library(testthat)

test_that("generate_abc_sample_3 throws errors for bad parameters", {
  
  expect_error(generate_abc_sample_3(given_data1, given_data2, avg_frob_distance, 
                    prior_distribution_3, simulate_household_data, epsilon = -1)) # negative epsilon
  expect_error(generate_abc_sample_3(given_data1, given_data2, avg_frob_distance, 
                    prior_distribution_3, simulate_household_data, epsilon = 0)) # zero epsilon
})

```


```{r}
test_that("generate_abc_sample_3 throws error for invalid prior distribution format", {
  expect_error(generate_abc_sample_3(given_data3, given_data4, avg_frob_distance, 
                    function() "invalid", simulate_household_data, epsilon = 10)) # Non-list, non-vector prior
  expect_error(generate_abc_sample_3(given_data3, given_data4, avg_frob_distance, 
                    function() list(), simulate_household_data, epsilon = 10)) # Empty list
})
```

```{r}
test_that("generate_abc_sample_3 throws error for improperly named list elements", {
    # Custom prior_distribution returning an incorrectly named list
    bad_prior_distribution <- function() {
        return(list(q_a = 0.5, q_x = 0.3, q_z = 0.9)) # Incorrect names
    }

    expect_error(
        generate_abc_sample_3(given_data3, given_data4, avg_frob_distance, 
            bad_prior_distribution, simulate_household_data, epsilon = 100
        )
    )
})
```

```{r}
test_that("generate_abc_sample_3 throws error for invalid prior distribution length", {
    # Custom prior_distribution returning an incorrectly named list
    bad_prior_distribution <- function() {
        return(c(0.1, 0.2)) # Incorrect length
    }

    expect_error(
        generate_abc_sample_3(given_data3, given_data4, avg_frob_distance, 
            bad_prior_distribution, simulate_household_data, epsilon = 100
        )
    )
})
```

```{r}
test_that("generate_abc_sample_3 handles zero values in prior distribution", {
  result <- generate_abc_sample_3(given_data3, given_data4, avg_frob_distance, 
                                function() c(0, 0, 0), 
                                simulate_household_data, epsilon = 100) 
  expect_equal(result[1], 0)       # Check that the first value is 0
  expect_equal(result[2], 0)       # Check that the second value is 0
})
```


### Tests for generate_abc_sample_combined

```{r}
# Tests for generate_abc_sample_combined

library(testthat)

given_data <- combined_influenza_data  #dummy data

test_that("generate_abc_sample_combined throws errors for bad parameters", {
  
  expect_error(generate_abc_sample_combined(given_data, frob_distance, 
                    prior_distribution_2, simulate_household_data, epsilon = -1)) # Negative epsilon
  expect_error(generate_abc_sample_combined(given_data, frob_distance, 
                    prior_distribution_2, simulate_household_data, epsilon = 0)) # Zero epsilon
})

test_that("generate_abc_sample_combined throws error for invalid prior distribution format", {
  expect_error(generate_abc_sample_combined(given_data, frob_distance, 
                    function() "invalid", simulate_household_data, epsilon = 10)) # Non-list, non-vector prior
  expect_error(generate_abc_sample_combined(given_data, frob_distance, 
                    function() list(), simulate_household_data, epsilon = 10)) # Empty list
})

test_that("generate_abc_sample_combined throws error for improperly named list elements", {
    # Custom prior_distribution returning an incorrectly named list
    bad_prior_distribution <- function() {
        return(list(q_x = 0.5, q_y = 0.3)) # Incorrect names
    }

    expect_error(generate_abc_sample_combined(given_data, frob_distance, 
            bad_prior_distribution, simulate_household_data, epsilon = 100))
})

test_that("generate_abc_sample_combined throws error for invalid prior distribution length", {
    # Custom prior_distribution returning a list/vector with incorrect length
    bad_prior_distribution <- function() {
        return(c(0.1)) # Incorrect length
    }

    expect_error(generate_abc_sample_combined(given_data, frob_distance, 
            bad_prior_distribution, simulate_household_data, epsilon = 100))
})

```


## Bibliography

[1] Toni T, Stumpf M.P.H. Simulation-based model selection for dynamical systems in systems and population biology. Bioinformatics, 104–110, 2010.

[2] Toni T, Stumpf M.P.H. Supplementary figures and datasets to [1]. 

[3] Addy C, Jr IL and Haber M. A generalized stochastic model for the analysis of infectious disease final size data. Biometrics, 961–974, 1991.

[4] Jr IL and Koopman J. Household and community transmission parameters from final distributions of infections in households. Biometrics, 115–126, 1982.