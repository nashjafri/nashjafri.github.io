---
title: "Approximate Bayesian Computation for Disease Outbreak"
author: 
  - "Nasheed Jafri"
  - "Abigail Watkins"
output: html_document
---

## Introduction

In this project, we apply ABC to fit parameters in a model for influenza A and B strains, based on data from past outbreaks in Michigan and Seattle. The process involves drawing parameters from prior uniform distribution, simulating data based on those parameters, computing a similarity measure between simulated and observed data, and iteratively refining the parameter estimates. 

**Objective**: The objective of this project is to replicate the model selection results presented in Section 3.3 of Tony and Stumpf's paper, "[Simulation-based model selection for dynamical systems in systems and population biology](https://watermark.silverchair.com/bioinformatics_26_1_104.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA3swggN3BgkqhkiG9w0BBwagggNoMIIDZAIBADCCA10GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMjV15kHzFknWstbVZAgEQgIIDLvGZcFAgMxJ2FtxTGrsPvkAoO-imsFvwyY2RJRbPWpz_WYOR3ZYIIXnJpmCp_pOOlhB9fGwvPCCBNkFN7jjQvo-jtSs3vYGT9U9ABO1ngcGxGq0M_-xfz5QDNcfLJpdHphXjvXPmNQKw-FbmW7Z-lM4VADhWgRMXeAa69IcQWbf3O3M4YVlAfOhNibTRLt8QLpayutZlbZAwX6aC2a13wmjnKF6Vx3WJWazbewssqJov9CmNXprFKUqnhcq1QLZ4oaGSKYaxVFpmwB2ZylzUBbliQ3fYN6VRAfleLXrmyvOymid2GtXNnhrslyx6SN2OSgbXU0YIgfSgCk5OaCETsFY7VMGzLCuUTB776n6hDJKcZ-Hb7RelIJxLeOZteaxRVOiu-a9pG5NbQQuueQtS0C-kqHlVksEwUAucqzS9UXX3ucvmsIgYK-jQQ8jmtqPTjVkdFGhR1J3LzOw7VJCJQy4b_a_WZLDNS7bskxvvZgU7DOZAVHxYu1aPUHh3UaeJ-5oMwJ-sqFWg_6ZruUPk4L9f1KB1siRgSmxw-Eo4JHKXjSEsIXAylD3m_trgxEIxkeqgXFJ867U-qJxeG39ToS9BptAG_IGK-HfMD0ovPK9mKHXvrp32fRO5S0oiqaCMa8kV4DGwbZjaMArJDV9Ps3WNw_EE2E8m7J4UjiqLNQkihUtUM6d4xmJ-S4zo-qPJkr0ajWkDhQwkeJ1wsaYGXItivcoAB4lzyQmG3Zs5kQQIIa2m4hveEf2mDlglHMoPHTAN5hGG-9_LegexhFcKAZTguF4nNpozqAVsIQaj8DeAaHWY8AvjP5HjDTgYHs4ni3w7EjULGDSroFhBndTpCAMNjtY9yIqoh248Bf7ayWtBCXUx1yJyIamAPGeHej3nPnf80TACr2Of6fJicQ-hFcdVGzQj8qiq8b9GuOFFJ43SnZudftdAwwlA0mQb30ZhvgJvsYngGY752-NegVEd2F_r6N2Jkw-G-fzEnoObf6OXzGaYlNSq4_s4vGnQ38HGe_HPk7fdDIESUA35j1SbK5K83274IWvReND0Byubb2MmgZ4fJ90sCCKnjLeu9ho)" and demonstrate how ABC can be used to estimate model parameters for complex epidemic models. In particular, we recreate Figures 3(a) and 3(c) using a 4 parameter model on two sets of observed data - Table 2 (Addy et al., 1991) and Table 3 (Longini and Koopman, 1982) from the [Supplementary Data](https://academic.oup.com/bioinformatics/article/26/1/104/182571).

## ABC using the 4-parameter model

Let $q_c$ denote the probability that a susceptible individual does not get infected from the community and $q_h$ the probability that a susceptible individual escapes infection within their household. The probability $w_{js}$, that $j$ out of the $s$ susceptible individuals in a household become infected, is then given by

$$w_{js} = \binom{s}{j}w_{jj}(q_cq_h^j)^{s-j}\, \, , s = 1,2,\dots \, \, , j = 0, 1, \dots , s$$
where $w_{0s} = q_c^s$ for $s = 0,1,2,\dots$ and $w_{ss} = 1 - \sum_{j = 0}^{s-1}w_{js}$.

We aim to infer the pair of parameters, $q_h$ and $q_c$, from the model using data from Supplementary Tables 2 and 3. In this project, we focus on the 4-parameter model with parameters $(q_{c1}, q_{h1}, q_{c2}, q_{h2})$, which represents the hypothesis that each outbreak has its own infection-transmission rates.

### Prior distribution

Prior distributions of all parameters are chosen to be uniform over the range [0,1].

**Prior distribution:** $q_{c1}, q_{h1}, q_{c2}, q_{h2} \sim \text{Uniform}[0,1]$

In the following code, we pick 4 independent parameters from the distribution Uniform[0,1]. This returns a vector of length 4, which we use to later extract parameters $q_{c1}, q_{h1}, q_{c2}, q_{h2}$.

```{r}
prior_distribution <- function() runif(n = 4, min = 0, max = 1)    
```

### Probability distribution $w_{js}$ 

We define a function that takes parameters $q_c$, $q_h$ and (maximum) household size, and returns the probability distribution matrix `w_js_matrix` with entries $w_{js}$ for $j = 0,1,\dots, s$, $s = 1 ,2, \dots,$household_size. 

```{r}
# Function to compute w_js matrix for a given outbreak
W_js_matrix <- function(q_c, q_h, household_size) {
  
  # Throw error if q_c or q_h is not a probability
  if ((q_c < 0) || (q_h < 0) || (q_c > 1) || (q_h > 1)) {
    stop(paste("Invalid inputs:", "q_c =", q_c, "b =", q_h, "; Probabilities must be between 0 and 1."))
  }
  
  nCol <- household_size      # Columns (susceptible individuals) indexed by s = 1,..., household_size
  nRow <- household_size + 1  # Rows (infected individuals) indexed by j = 0, 1,..., s
  
  # Initialize a (zero) matrix to store the probabilities w_js
  w_js_matrix <- matrix(0, nrow = nRow , ncol = nCol)
  
  for (s in 1:nCol) {  # Iterate over number of susceptible individuals in a household
    for (j in 0:s) {   # Iterate over number of infected individuals
                       # j corresponds to row (j + 1) of the w_js matrix
      if (j == 0) {    
          w_js_matrix[j + 1,s] <- q_c^s   
      } else if (j < s) {                 
          w_js_matrix[j + 1, s] <- choose(s, j) * w_js_matrix[j + 1, j] * (q_c * q_h^j)^(s - j)
      } else {                           
          w_js_matrix[j + 1, s] <- 1 - sum(w_js_matrix[1:j,s])
      }
    }
  }
  
  rownames(w_js_matrix) <- paste("j =", 0:household_size)
  colnames(w_js_matrix) <- paste("s =", 1:household_size)
  
  return(w_js_matrix)
}
```

### Simulating data from the distribution $w_{js}$

In this section, we simulate data based on the probability distribution $w_{js}$. The function below takes as inputs the parameters $q_c$, $q_h$, (maximum) household size, and the number of households (for each household size, as a vector), and returns a matrix of simulated data. The simulation is carried out by sampling the number of infected individuals for each household size and store their counts in a matrix.

```{r}
simulate_household_data <- function(q_c, q_h, household_size, n_households) {
  
  # Throw error if user doesn't provide number of households for each household size
  if (length(n_households) != household_size) {
    stop("Invalid inputs: length(n_households) does not match (maximum) household_size.")
  }
  
  w_js_matrix <- W_js_matrix(q_c, q_h, household_size)
  
  nCol <- household_size      # Columns (susceptible individuals) indexed by s = 1,..., household_size
  nRow <- household_size + 1  # Rows (infected individuals) indexed by j = 0, 1,..., s
  
  # Initialize a (zero) matrix to store the simulated data
  simulated_data <- matrix(0, nrow = nRow, ncol = nCol)
  
  for (s in 1:nCol) {                           
                                          
    probabilities <- w_js_matrix[1:(s + 1), s]    # (j+1) corresponds to row j of w_js_matrix
    
    # Simulate household infections using random sampling
    samples <- sample(0:s, size = n_households[s], replace = TRUE, prob = probabilities)
    
    # Count the occurrences of each infection level
    counts <- table(factor(samples, levels = 0:s))
    
    # Store the simulated counts in the matrix
    simulated_data[1:(s + 1), s] <- as.numeric(counts)
  }
  
  rownames(simulated_data) <- paste("(Infected) j =", 0:household_size)
  colnames(simulated_data) <- paste("s =", 1:household_size)
  
  return(simulated_data)
}
```

We use the `sample` function to generate the samples, but the `rmultinom` function can also be used for the same purpose. 

**Example:** The following is an example of one simulated dataset, given parameters $q_c = 0.8$, $q_h = 0.6$, maximum household size of 5 and number of households same as in Table 2:

```{r}
q_c <- 0.8                                # Probability of avoiding community infection
q_h <- 0.6                                # Probability of escaping household infection
household_size <- 5                       # Maximum number of susceptible individuals in a household
n_households <- c(79, 105, 48, 44, 11)    # Number of households (same as observed data in Table 2)

simulated_data <- simulate_household_data(q_c, q_h, household_size, n_households)
print(simulated_data)
```
### Similarity measure between the simulated and observed data

To apply ABC, we use the following distance function for observed and simulated datasets.

$$d(D_{obs}, D^*) = \cfrac{1}{2}\left(\|D_1−D^∗(q_{c1},q_{h1}) \|_F + \| D_2−D^∗(q_{c2},q_{h2}) \|_F \right) \, ,$$
where 

- $D_{obs} = D_1 \cup D_2$ with $D_1$ the 1977–1978 outbreak and $D_2$ the 1980–1981 outbreak datasets from Supplementary Table 2
- $D^*$ is the simulated data
- $\| \, \|_F$ denotes the Frobenius norm of a matrix defined by $\|A\|_F = \sqrt{\sum_{i,j}|a_{ij}|^2}$.

We define the said distance function that takes two pairs of observed and simulated data and returns the average Frobenius distance between them. 

```{r}
frobenius_norm <- function(A) return(sqrt(sum(A^2)))
distance <- function(given_data1, given_data2, simulated_data1, simulated_data2){
  distance1 = frobenius_norm(given_data1-simulated_data1)
  distance2 = frobenius_norm(given_data2-simulated_data2)
  total_distance = (distance1+distance2)/2
  return(total_distance)
}
```

### Generating ABC samples

The ABC algorithm for this project is given below.

**Inputs:**

* 4-parameter vector: $q = (q_{c1}, q_{h1}, q_{c2}, q_{h2})$

* Target posterior: $P(q|D_{obs}) \propto P(D_{obs} | q)P(q)$

* A way of simulating from $P(D_{obs}|q) \sim w_{js}$

* Prior on the parameters: $P(q) \sim \text{Uniform}[0,1]$

* Similarity measure: $d(D_{obs}, D^*) = \frac{1}{2} \left( \| D_1 - D^*(q_{c1}, q_{h1}) \|_F + \| D_2 - D^*(q_{c2}, q_{h2}) \|_F \right)$

* Tolerance $\epsilon$

**Sampling:** for $i = 1, 2, \dots, N$

* Generate $q^{(i)} \sim \text{Uniform}[0,1]$

* Generate $D^{*(i)} \sim w_{js}$

**Accept/Reject Criterion:**

* If $d(D_{obs}, D^{*(i)}) < \epsilon$, accept $q^{(i)}$, else reject.

**Posterior Approximation:**

* The accepted parameter values approximate the posterior distribution $P(q|D_{obs})$.

#### Function to Generate Posterior Samples

Next, we generate posterior samples by defining a function that takes the pair of observed data (given_data1, given_data2), a similarity measure (distance), a prior distribution for the parameter, a data-simulating function, and a tolerance level. This function returns a posterior sample of the parameter $q$ (in our case, the 4-parameter vector $(q_{c1}, q_{h1}, q_{c2}, q_{h2})$) that satisfies the tolerance condition for the distance between simulated and given data.

```{r}
generate_abc_sample <- function(given_data1, given_data2,
                                distance,
                                prior_distribution,
                                data_simulating_function,
                                epsilon) {
    while(TRUE) {
        q <- prior_distribution()
        
        q_c1 <- q[1]
        q_h1 <- q[2]
        q_c2 <- q[3]
        q_h2 <- q[4]
        
        household_size1 <- ncol(given_data1)
        n_households1 <- colSums(given_data1)
        
        simulated_data1 <- data_simulating_function(q_c1, q_h1, household_size1, n_households1)
        
        household_size2 <- ncol(given_data2)
        n_households2 <- colSums(given_data2)
        
        simulated_data2 <- data_simulating_function(q_c2, q_h2, household_size2, n_households2)
        
        if(distance(given_data1, given_data2, simulated_data1, simulated_data2) < epsilon) {
            return(q)
        }
    }
}
```

## Observed Data (Table 2)

The following code stores the observed data from Table 2 of the Supplementary Material. 

- given_data1: Influenza A (H3N2) infection in 1977-78, Tecumseh, Michigan. [3]
- given_data2: Influenza A (H3N2) infection in 1980-81, Tecumseh, Michigan. [3]  

```{r}
given_data1 <- matrix(c(66, 87, 25, 22, 4,
                        13, 14, 15, 9, 4,
                        0, 4, 4, 9, 1,
                        0, 0, 4, 3, 1,
                        0, 0, 0, 1, 1, 
                        0, 0, 0, 0, 0), byrow = TRUE, ncol=5)

given_data2 <- matrix(c(44, 62, 47, 38, 9,
                        10, 13, 8, 11, 5,
                        0, 9, 2, 7, 3,
                        0, 0, 3, 5, 1,
                        0, 0, 0, 1, 0, 
                        0, 0, 0, 0, 1), byrow = TRUE, ncol=5)

```

### Applying ABC to get posterior samples from Observed Data from Table 2

#### Note about tolerance level $\epsilon$ 

Before applying ABC to get sample parameters, we make a small note about what is considered a reasonable tolerance level. Our datasets represent counts of households with $s$ susceptible and $j$ infected individuals, structured as a 6×5 matrix. **By design, these counts are always integers**. Notably, 10 entries in each dataset are fixed to zero, corresponding to cases where $j>s$. The remaining 20 entries are randomly generated using the simulate_household_data function.

Since both observed data and simulated data are 6×5 matrices with integer entries, and 10 of the entries are always zero, the Frobenius distance can be sensitive to small differences. For instance, if 5 of the 20 entries are identical and remaining 15 differ by only 1, the Frobenius distance becomes 15. This scale justifies setting $\epsilon = 15$ as a reasonable threshold, which is what we use to generate the posteriors.

Now we are ready to apply our ABC sample generating function to get posterior samples for $(q_{c1}, q_{h1}, q_{c2}, q_{h2})$.

```{r}
prior_distribution <- function() runif(n = 4, min = 0, max = 1)     
# Generating one posterior sample
generate_abc_sample(given_data1, given_data2, distance,
                        prior_distribution, simulate_household_data, epsilon = 15)
```

The code below produces 100 parameter samples that are accepted by the algorithm with $\epsilon = 15$. This is a computationally expensive process and took about 2 hours to run on RStudio.

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation, eval=FALSE}
# Generating 100 posterior samples
posterior_samples <- replicate(n = 100,
    generate_abc_sample(given_data1, given_data2, distance,
                        prior_distribution, simulate_household_data, epsilon = 15))
```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save posterior_samples_Table2.rds on your local machine (from Drive) before running this
# saveRDS(posterior_samples, "posterior_samples_Table2.rds")
posterior_samples <- readRDS("posterior_samples_Table2.rds")
```

Here are a few posterior samples from the 100 generated above.

```{r}
posterior_samples[, 95:100]
```

We now extract individual parameters $q_{c1}$, $q_{h1}$, $q_{c2}$ and $q_{h2}$ from the samples. 

```{r}
# Extracting samples of parameters q_c1, q_h1, q_c2, q_h2 from the posterior samples
q_c1 <- posterior_samples[1, ]  # First row for q_c1
q_h1 <- posterior_samples[2, ]  # Second row for q_h1
q_c2 <- posterior_samples[3, ]  # Third row for q_c2
q_h2 <- posterior_samples[4, ]  # Fourth row for q_h2
```

Finally, we plot the simulated parameters $q_h$ vs $q_c$ for the two outbreaks. This recreates Figure 3(a) from [Toni and Stumpf (2010)](https://watermark.silverchair.com/bioinformatics_26_1_104.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA3swggN3BgkqhkiG9w0BBwagggNoMIIDZAIBADCCA10GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMjV15kHzFknWstbVZAgEQgIIDLvGZcFAgMxJ2FtxTGrsPvkAoO-imsFvwyY2RJRbPWpz_WYOR3ZYIIXnJpmCp_pOOlhB9fGwvPCCBNkFN7jjQvo-jtSs3vYGT9U9ABO1ngcGxGq0M_-xfz5QDNcfLJpdHphXjvXPmNQKw-FbmW7Z-lM4VADhWgRMXeAa69IcQWbf3O3M4YVlAfOhNibTRLt8QLpayutZlbZAwX6aC2a13wmjnKF6Vx3WJWazbewssqJov9CmNXprFKUqnhcq1QLZ4oaGSKYaxVFpmwB2ZylzUBbliQ3fYN6VRAfleLXrmyvOymid2GtXNnhrslyx6SN2OSgbXU0YIgfSgCk5OaCETsFY7VMGzLCuUTB776n6hDJKcZ-Hb7RelIJxLeOZteaxRVOiu-a9pG5NbQQuueQtS0C-kqHlVksEwUAucqzS9UXX3ucvmsIgYK-jQQ8jmtqPTjVkdFGhR1J3LzOw7VJCJQy4b_a_WZLDNS7bskxvvZgU7DOZAVHxYu1aPUHh3UaeJ-5oMwJ-sqFWg_6ZruUPk4L9f1KB1siRgSmxw-Eo4JHKXjSEsIXAylD3m_trgxEIxkeqgXFJ867U-qJxeG39ToS9BptAG_IGK-HfMD0ovPK9mKHXvrp32fRO5S0oiqaCMa8kV4DGwbZjaMArJDV9Ps3WNw_EE2E8m7J4UjiqLNQkihUtUM6d4xmJ-S4zo-qPJkr0ajWkDhQwkeJ1wsaYGXItivcoAB4lzyQmG3Zs5kQQIIa2m4hveEf2mDlglHMoPHTAN5hGG-9_LegexhFcKAZTguF4nNpozqAVsIQaj8DeAaHWY8AvjP5HjDTgYHs4ni3w7EjULGDSroFhBndTpCAMNjtY9yIqoh248Bf7ayWtBCXUx1yJyIamAPGeHej3nPnf80TACr2Of6fJicQ-hFcdVGzQj8qiq8b9GuOFFJ43SnZudftdAwwlA0mQb30ZhvgJvsYngGY752-NegVEd2F_r6N2Jkw-G-fzEnoObf6OXzGaYlNSq4_s4vGnQ38HGe_HPk7fdDIESUA35j1SbK5K83274IWvReND0Byubb2MmgZ4fJ90sCCKnjLeu9ho) using the Supplementary data in Table 2. As in the original paper, we use \textcolor{red}{red} for the 1977 outbreak ($q_{h1}, q_{c1}$) and \textcolor{blue}{blue} for the 1980 outbreak ($q_{h2}, q_{c2}$).


```{r}
# Recreate Figure 3(a) with n = 100 posterior samples and epsilon = 15

par(pty = "s")      

plot(q_h1, q_c1, 
     main = "q_h vs q_c (n = 100, epsilon = 15)", 
     xlab = "q_h", ylab = "q_c", 
     xlim = c(0, 1), ylim = c(0, 1), 
     pch = 16, col = "red", 
     cex = 1.5)  

# Add second set of data (q_h2, q_c2) in blue
points(q_h2, q_c2, pch = 16, col = "blue", cex = 1.2)

# Add a legend
legend("bottomleft", 
       legend = c("given_data1 from Table 2", "given_data2 from Table 2"), 
       col = c("red", "blue"), 
       pch = 16, bty = "n")
```

## Observed Data (Table 3)

The following code stores the observed data from Table 3 of the Supplementary Material. 

- given_data3: Influenza B infection in 1975-76, Seattle, Washington. [4]
- given_data4: Influenza A (H1N1) infection in 1978-79, Seattle, Washington. [4]  

```{r}
given_data3 <- matrix(c(9, 12, 18, 9, 4,
                        1, 6, 6, 4, 3,
                        0, 2, 3, 4, 0,
                        0, 0, 1, 3, 2,
                        0, 0, 0, 0, 0, 
                        0, 0, 0, 0, 0), byrow = TRUE, ncol=5)

given_data4 <- matrix(c(15, 12, 4,
                        11, 17, 4,
                        0, 21, 4,
                        0, 0, 5 ), byrow = TRUE, ncol=3)

```

### Applying ABC to get posterior samples from Observed Data from Table 3

As with data from Table 2, we now apply our sample generating function to get posterior samples for $(q_{c3}, q_{h3}, q_{c4}, q_{h4})$. Since the given data in Table 3 is smaller than the one in Table 2, we are able to choose a smaller $\epsilon$ and still be within computational limits. We use $\epsilon = 7$ as a reasonable tolerance level.

```{r}
prior_distribution <- function() runif(n = 4, min = 0, max = 1)     

# Generating one posterior sample 

generate_abc_sample(given_data3, given_data4, distance, 
                    prior_distribution, simulate_household_data, epsilon = 7)
```

The code below produces 200 parameter samples that are accepted by the algorithm with $\epsilon = 7$.

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation-2, eval=FALSE}
# Generating 200 posterior samples
posterior_samples2 <- replicate(n = 200,
    generate_abc_sample(given_data3, given_data4, distance, 
                        prior_distribution, simulate_household_data, epsilon = 7))
```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save posterior_samples_Table3.rds on your local machine (from Drive) before running this
# saveRDS(posterior_samples2, "posterior_samples_Table3.rds")
posterior_samples2 <- readRDS("posterior_samples_Table3.rds")
```

Here are a few posterior samples from the 200 generated above.

```{r}
posterior_samples2[, 95:100]
```

We now extract individual parameters $q_{c3}$, $q_{h3}$, $q_{c4}$ and $q_{h4}$ from the samples. 

```{r}
# Extracting samples of parameters q_c3, q_h3, q_c4, q_h4 from the posterior samples
q_c3 <- posterior_samples2[1, ]  # First row for q_c3
q_h3 <- posterior_samples2[2, ]  # Second row for q_h3
q_c4 <- posterior_samples2[3, ]  # Third row for q_c4
q_h4 <- posterior_samples2[4, ]  # Fourth row for q_h4
```

Finally, we plot the simulated parameters $q_h$ vs $q_c$ for the two outbreaks. This recreates Figure 3(b) from [Toni and Stumpf (2010)](https://watermark.silverchair.com/bioinformatics_26_1_104.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA3swggN3BgkqhkiG9w0BBwagggNoMIIDZAIBADCCA10GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMjV15kHzFknWstbVZAgEQgIIDLvGZcFAgMxJ2FtxTGrsPvkAoO-imsFvwyY2RJRbPWpz_WYOR3ZYIIXnJpmCp_pOOlhB9fGwvPCCBNkFN7jjQvo-jtSs3vYGT9U9ABO1ngcGxGq0M_-xfz5QDNcfLJpdHphXjvXPmNQKw-FbmW7Z-lM4VADhWgRMXeAa69IcQWbf3O3M4YVlAfOhNibTRLt8QLpayutZlbZAwX6aC2a13wmjnKF6Vx3WJWazbewssqJov9CmNXprFKUqnhcq1QLZ4oaGSKYaxVFpmwB2ZylzUBbliQ3fYN6VRAfleLXrmyvOymid2GtXNnhrslyx6SN2OSgbXU0YIgfSgCk5OaCETsFY7VMGzLCuUTB776n6hDJKcZ-Hb7RelIJxLeOZteaxRVOiu-a9pG5NbQQuueQtS0C-kqHlVksEwUAucqzS9UXX3ucvmsIgYK-jQQ8jmtqPTjVkdFGhR1J3LzOw7VJCJQy4b_a_WZLDNS7bskxvvZgU7DOZAVHxYu1aPUHh3UaeJ-5oMwJ-sqFWg_6ZruUPk4L9f1KB1siRgSmxw-Eo4JHKXjSEsIXAylD3m_trgxEIxkeqgXFJ867U-qJxeG39ToS9BptAG_IGK-HfMD0ovPK9mKHXvrp32fRO5S0oiqaCMa8kV4DGwbZjaMArJDV9Ps3WNw_EE2E8m7J4UjiqLNQkihUtUM6d4xmJ-S4zo-qPJkr0ajWkDhQwkeJ1wsaYGXItivcoAB4lzyQmG3Zs5kQQIIa2m4hveEf2mDlglHMoPHTAN5hGG-9_LegexhFcKAZTguF4nNpozqAVsIQaj8DeAaHWY8AvjP5HjDTgYHs4ni3w7EjULGDSroFhBndTpCAMNjtY9yIqoh248Bf7ayWtBCXUx1yJyIamAPGeHej3nPnf80TACr2Of6fJicQ-hFcdVGzQj8qiq8b9GuOFFJ43SnZudftdAwwlA0mQb30ZhvgJvsYngGY752-NegVEd2F_r6N2Jkw-G-fzEnoObf6OXzGaYlNSq4_s4vGnQ38HGe_HPk7fdDIESUA35j1SbK5K83274IWvReND0Byubb2MmgZ4fJ90sCCKnjLeu9ho) using the Supplementary data in Table 3. As in the original paper, we use \textcolor{red}{red} for the 1975 outbreak ($q_{h3}, q_{c3}$) and \textcolor{blue}{blue} for the 1978 outbreak ($q_{h4}, q_{c4}$).


```{r}
# Recreate Figure 3(b) with n = 200 posterior samples and epsilon = 7

par(pty = "s")      

plot(q_h3, q_c3, 
     main = "q_h vs q_c (n = 200, epsilon = 7)", 
     xlab = "q_h", ylab = "q_c", 
     xlim = c(0, 1), ylim = c(0, 1), 
     pch = 16, col = "red", 
     cex = 1.5)  

# Add second set of data (q_h4, q_c4) in blue
points(q_h4, q_c4, pch = 16, col = "blue", cex = 1.2)

# Add a legend
legend("bottomleft", 
       legend = c("given_data3 from Table 3", "given_data4 from Table 3"), 
       col = c("red", "blue"), 
       pch = 16, bty = "n")
```

## Conclusion 
xxxxxxxxxx

## Appendix : Function Tests

### Tests for W_js_matrix

```{r}
# Tests for W_js_matrix

library(testthat)

test_that("W_js_matrix throws error for bad parameters", {
  expect_error(W_js_matrix(0.5, -0.1, 4))        # Negative q_h
  expect_error(W_js_matrix(2, 0.9, 3))           # q_c > 1
  expect_error(W_js_matrix(0.3, 0.2, -7))        # household_size not a positive integer
})

test_that("W_js_matrix works for simple cases", {
  # q_h = q_c = 0 
  W0 <- W_js_matrix(0,0,3)
  expected_W0 <- matrix(c(0,0,0,
                         1,0,0,
                         0,1,0,
                         0,0,1), ncol = 3, byrow = T)
  expect_equivalent(W0, expected_W0)            # expect_equal will fail because of attribute comparison 
                                                # W0 has rownames and colnames, while expected_W0 does not
  # q_h = q_c = 1 
  W1 <- W_js_matrix(1,1,4)
  expected_W1 <- matrix(c(1,1,1,1,
                         0,0,0,0,
                         0,0,0,0,
                         0,0,0,0,
                         0,0,0,0), ncol = 4, byrow = T)
  expect_equivalent(W1, expected_W1)            # W1 has rownames and colnames, while expected_W1 does not
})
  
test_that("W_js_matrix columns sum to 1", {
  for (i in 1:100) {                            # Test for 100 random cases
    q_c <- runif(1, 0, 1)                       # Choose q_c randomly between 0 and 1
    q_h <- runif(1, 0, 1)                       # Choose q_h randomly between 0 and 1
    household_size <- sample(1:10, 1)           # Choose household_size randomly between 1 and 10

    W <- W_js_matrix(q_c, q_h, household_size)
    
    expect_equivalent(colSums(W), rep(1, ncol(W)))   
  }
})
```


### Tests for simulate_household_data

```{r}
# Tests for simulate_household_data

library(testthat)

test_that("simulate_household_data throws error for bad parameters", {
  expect_error(simulate_household_data(0.5, -0.1, 3, c(12,10,6)))   # Negative q_h
  expect_error(simulate_household_data(1.4, 0.8, 4, c(12,10,6,3)))  # q_c > 1
  expect_error(simulate_household_data(0.3, 0.4, 6, c(100,90,8)))   # length(n_households) != household_size 
})

test_that("simulate_household_data works for simple cases", {
  # 0 number of households for household size 3 should return third column 0
  data.0 <- simulate_household_data(0.1, 0.8, 4, c(12,8,0,10))

  expect_equivalent(data.0[,3], rep(0,nrow(data.0)))  
})
  
test_that("simulate_household_data doesn't have more infected than susceptible", {
  q_c <- runif(1, 0, 1)                          # Choose q_c randomly between 0 and 1
  q_h <- runif(1, 0, 1)                          # Choose q_h randomly between 0 and 1
  household_size <- sample(1:10, 1)              # Random household size between 1 and 10
  n_households <- sample(1:100, household_size)  # Random n_households vector of size household_size 
  
  result <- simulate_household_data(q_c, q_h, household_size, n_households)
  
  # Validate that entries for j > s are 0
  for (j in 1 : nrow(result)) {             
    for (s in 1 : ncol(result)) {           
      if (j-1 > s) {                             # j corresponds to row (j-1)
        expect_equivalent(result[j, s], 0)    
      }
    }
  }
})
```


## Bibliography

[1] Toni T, Stumpf M.P.H. Simulation-based model selection for dynamical systems in systems and population biology. Bioinformatics, 104–110, 2010.

[2] Toni T, Stumpf M.P.H. Supplementary figures and datasets to [1]. 

[3] Addy C, Jr IL and Haber M. A generalized stochastic model for the analysis of infectious disease final size data. Biometrics, 961–974, 1991.

[4] Jr IL and Koopman J. Household and community transmission parameters from final distribu- tions of infections in households. Biometrics, 115–126, 1982.